{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7WfMVrriSGtE",
    "outputId": "31a75902-2b42-4f0b-f44e-b22e8ee807bf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from deepchem.feat import PagtnMolGraphFeaturizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from rdkit.Chem import AllChem as Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv, global_max_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwTBWMKVUh_2",
    "outputId": "38f14995-5186-4b79-a6d7-ce9efcddf426"
   },
   "outputs": [],
   "source": [
    "mol = pd.read_csv(\"/result.csv\")\n",
    "\n",
    "smiles = mol.drop_duplicates(subset = ['Pubid'], keep='first')\n",
    "smiles = smiles['Smiles']\n",
    "\n",
    "smiles_counts = mol['Smiles'].value_counts().reset_index()\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "# Initialize the featurizer\n",
    "featurizer = PagtnMolGraphFeaturizer(max_length=5)\n",
    "\n",
    "# Apply the featurizer to the SMILES series\n",
    "featurized_data = featurizer.featurize(smiles.tolist())\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the featurizer\n",
    "featurizer = PagtnMolGraphFeaturizer(max_length=5)\n",
    "\n",
    "# Apply the featurizer to the SMILES series\n",
    "featurized_data = featurizer.featurize(smiles.tolist())\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, graph_data in enumerate(featurized_data):\n",
    "    result = {\n",
    "        \"SMILES\": smiles.iloc[i],\n",
    "        \"Node Features\": graph_data.node_features.tolist(),  # Convert to list for storage\n",
    "        \"Edge Features\": graph_data.edge_features.tolist(),   # Convert to list for storage\n",
    "        \"Edge Index\": graph_data.edge_index.tolist()         # Convert to list for storage\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df.to_csv(\"node_edge_features.csv\", index=False)\n",
    "\n",
    "# Display the results\n",
    "print(results_df.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDebSSwUW89C"
   },
   "outputs": [],
   "source": [
    "df = pd.merge(results_df, smiles_counts, left_on='SMILES', right_on='Smiles', how = 'left')\n",
    "(df['count']).describe()\n",
    "\n",
    "unique_interactions = mol.groupby(\"Compound ID\")[\"gene_name\"].nunique()\n",
    "\n",
    "# Display results\n",
    "df_dedup = unique_interactions.reset_index()\n",
    "df['number_dedup'] = df_dedup['gene_name']\n",
    "df['count_binned_custom'] = pd.cut(df['number_dedup'], bins=[0, 16, 60.1, 115], labels=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yI1iynOtwV4a"
   },
   "source": [
    "##Test start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBsA1L3rwYYv",
    "outputId": "e8258a52-37ea-471a-8916-1c160eeea4b8"
   },
   "outputs": [],
   "source": [
    "df['count_binned_custom'] = pd.cut(df['number_dedup'], bins=[0, 20, 60.1, 115], labels=[0, 1, 2])\n",
    "graphs = []\n",
    "\n",
    "for i, graph_data in enumerate(featurized_data):\n",
    "    # Convert node features and edge indices into PyTorch tensors\n",
    "    x = torch.tensor(graph_data.node_features, dtype=torch.float)  # Node features\n",
    "    edge_index = torch.tensor(graph_data.edge_index, dtype=torch.long).t().contiguous()  # Edge index\n",
    "    edge_attr = torch.tensor(graph_data.edge_features, dtype=torch.float)  # Edge features\n",
    "    edge_index = edge_index.T\n",
    "    y = torch.tensor(df['count_binned_custom'][i], dtype=torch.long)  # Target label\n",
    "\n",
    "    # Create a PyTorch Geometric Data object\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "    print(data)\n",
    "\n",
    "    graphs.append(data)\n",
    "\n",
    "# Create a DataLoader\n",
    "loader = DataLoader(graphs, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "FMh1FNTuZQpA"
   },
   "outputs": [],
   "source": [
    "mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048)\n",
    "\n",
    "# Function to compute Morgan Fingerprints\n",
    "def compute_morgan_fingerprint(smiles, radius=2, fpSize=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        # Generate Morgan fingerprint\n",
    "        fingerprint = mfpgen.GetFingerprint(mol)\n",
    "        return np.array(fingerprint)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Have checked, no none\n",
    "df['morgan_fingerprint'] = df['SMILES'].apply(compute_morgan_fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "W2EP_cjXVf2y"
   },
   "outputs": [],
   "source": [
    "numeric_columns = ['Exact Mass', 'XLogP3', 'Heavy Atom Count', 'Ring Count',\n",
    "                   'Hydrogen Bond Acceptor Count', 'Hydrogen Bond Donor Count',\n",
    "                   'Rotatable Bond Count', 'Topological Polar Surface Area']\n",
    "\n",
    "# Additional columns to include (if any)\n",
    "additional_columns = []\n",
    "data = mol.drop_duplicates(subset = ['Pubid'], keep='first')\n",
    "new_df = pd.merge(data, df, left_on='Smiles', right_on='SMILES')\n",
    "# Combine the selected columns with the \"Features\" (fingerprints) to form the input data (X)\n",
    "df_combined = new_df[numeric_columns + additional_columns].copy()\n",
    "df_combined = df_combined.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "h2mmJ-3wVoH_"
   },
   "outputs": [],
   "source": [
    "df_full = pd.concat([df, df_combined], axis=1, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "s3U9CSSJat5L",
    "outputId": "1caa968a-29b7-44ad-ad64-60da0f3f1938"
   },
   "outputs": [],
   "source": [
    "additional_features_list = [\n",
    "    'Exact Mass', 'XLogP3', 'Heavy Atom Count',\n",
    "    'Ring Count', 'Hydrogen Bond Acceptor Count', 'Hydrogen Bond Donor Count',\n",
    "    'Rotatable Bond Count', 'Topological Polar Surface Area'\n",
    "]\n",
    "for col in additional_features_list:\n",
    "    unique_types = df_full[col].apply(type).value_counts()\n",
    "    print(f\"\\nColumn: {col} - Unique Data Types:\\n{unique_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "A15ieGT-X6pm",
    "outputId": "90951850-46c3-4aaf-b482-a7d5cd04654b"
   },
   "outputs": [],
   "source": [
    "graphs = []\n",
    "\n",
    "for i, graph_data in enumerate(featurized_data):\n",
    "    # Node-level features\n",
    "    node_features = torch.tensor(graph_data.node_features, dtype=torch.float)  # Original node features\n",
    "\n",
    "    # Graph-level features\n",
    "    morgan_fp = df_full.loc[i, 'morgan_fingerprint']  # Extract Morgan fingerprint\n",
    "    if isinstance(morgan_fp, np.ndarray):\n",
    "        morgan_fp = torch.tensor(morgan_fp.astype(np.float32), dtype=torch.float)\n",
    "    else:\n",
    "        raise TypeError(f\"Expected np.ndarray for 'morgan_fingerprint', got {type(morgan_fp)}\")\n",
    "\n",
    "    # Extract additional graph-level features from df_full\n",
    "    additional_features = torch.tensor(df_full.loc[i, additional_features_list], dtype=torch.float)\n",
    "\n",
    "    # Combine all graph-level features into a single tensor\n",
    "    graph_features = torch.cat([morgan_fp, additional_features], dim=0)  # Shape: [2048 + len(additional_features_list)]\n",
    "\n",
    "    # Edge indices and attributes\n",
    "    edge_index = torch.tensor(graph_data.edge_index, dtype=torch.long)\n",
    "    edge_attr = torch.tensor(graph_data.edge_features, dtype=torch.float)\n",
    "\n",
    "    # Target label\n",
    "    y = torch.tensor(df_full['count_binned_custom'][i], dtype=torch.long)\n",
    "\n",
    "    # Create a PyTorch Geometric Data object\n",
    "    data = Data(\n",
    "        x=node_features,              # Node-level features\n",
    "        edge_index=edge_index,        # Edge indices\n",
    "        edge_attr=edge_attr,          # Edge features\n",
    "        y=y,                          # Target label\n",
    "        graph_features=graph_features  # Graph-level features\n",
    "    )\n",
    "    print(data)\n",
    "    graphs.append(data)\n",
    "\n",
    "# Create a DataLoader\n",
    "loader = DataLoader(graphs, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gvOEYVNmiNu"
   },
   "outputs": [],
   "source": [
    "class GATWithGraphFeatures(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_edge_features, num_graph_features, hidden_channels, num_classes):\n",
    "        super(GATWithGraphFeatures, self).__init__()\n",
    "\n",
    "        # GAT layers for node features\n",
    "        self.conv1 = GATConv(num_node_features, hidden_channels, edge_dim=num_edge_features)\n",
    "        self.conv2 = GATConv(hidden_channels , hidden_channels, edge_dim=num_edge_features)\n",
    "\n",
    "        # Linear layers for node feature processing after pooling\n",
    "        self.node_lin1 = nn.Linear(hidden_channels, hidden_channels)\n",
    "\n",
    "        # Linear layers for graph-level features\n",
    "        self.graph_lin1 = nn.Linear(num_graph_features * 4, hidden_channels)\n",
    "        self.graph_lin2 = nn.Linear(hidden_channels, hidden_channels)\n",
    "\n",
    "        # Final linear layer combining node and graph features\n",
    "        self.final_lin = nn.Linear(hidden_channels * 2, num_classes)  # Combine node and graph embeddings\n",
    "\n",
    "        # Regularization layers\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.batch_norm_graph = nn.BatchNorm1d(hidden_channels)  # For graph features\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        graph_features = data.graph_features  # Graph-level features\n",
    "\n",
    "        # First graph convolution layer (with edge attributes)\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm1(x)\n",
    "\n",
    "        # Second graph convolution layer (with edge attributes)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm2(x)\n",
    "\n",
    "        # Global pooling to get graph-level representation from node features\n",
    "        node_embedding = global_max_pool(x, batch)  # Shape: [batch_size, hidden_channels * num_heads]\n",
    "        node_embedding = self.node_lin1(node_embedding)\n",
    "        node_embedding = F.relu(node_embedding)\n",
    "        node_embedding = self.dropout(node_embedding)\n",
    "\n",
    "        # Process graph-level features\n",
    "        graph_embedding = self.graph_lin1(graph_features)\n",
    "        graph_embedding = F.relu(graph_embedding)\n",
    "        graph_embedding = self.graph_lin2(graph_embedding)\n",
    "        graph_embedding = F.relu(graph_embedding)\n",
    "        graph_embedding = self.dropout(graph_embedding)\n",
    "\n",
    "        # Combine node and graph embeddings\n",
    "        graph_embedding_expanded = graph_embedding.unsqueeze(0).expand(node_embedding.size(0), -1)  # [32, 64]\n",
    "        combined = torch.cat([node_embedding, graph_embedding_expanded], dim=-1)  # Shape: [batch_size, hidden_channels * 2]\n",
    "\n",
    "        # Debugging shape before final linear layer\n",
    "\n",
    "        # Final classification layer\n",
    "        out = self.final_lin(combined)  # This should match the expected size\n",
    "        return F.log_softmax(out, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wBxIMoK36me"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GraphOnlyModel(nn.Module):\n",
    "    def __init__(self, num_graph_features, hidden_channels, num_classes):\n",
    "        super(GraphOnlyModel, self).__init__()\n",
    "        self.graph_lin1 = nn.Linear(num_graph_features, hidden_channels)\n",
    "        self.graph_lin2 = nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.out = nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.graph_features  # [batch_size, num_graph_features]\n",
    "\n",
    "        x = self.graph_lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm1(x)\n",
    "\n",
    "        x = self.graph_lin2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm2(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGLftpK3mZfL"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(graphs_subset, label):\n",
    "    train_subset, test_subset = torch.utils.data.random_split(\n",
    "        graphs_subset,\n",
    "        [train_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=4, shuffle=True, drop_last = True)\n",
    "    test_loader = DataLoader(test_subset, batch_size=4, drop_last = True)\n",
    "\n",
    "    model = GATWithGraphFeatures(\n",
    "        num_node_features=num_node_features,\n",
    "        num_edge_features=num_edge_features,\n",
    "        num_graph_features=len(graphs_subset[0].graph_features),\n",
    "        hidden_channels=64,\n",
    "        num_classes=3\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(30):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch)\n",
    "            loss = F.nll_loss(out, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    acc, prec, rec = evaluate_more(model, test_loader)\n",
    "    print(f\"[{label}] Test Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-6E1_XPugzF"
   },
   "outputs": [],
   "source": [
    "def remove_numeric_feature(graphs, numeric_index_to_remove):\n",
    "    \"\"\"\n",
    "    Removes a specific numeric feature from the graph_features tensor of each graph.\n",
    "\n",
    "    Parameters:\n",
    "    - graphs: list of PyG Data objects\n",
    "    - numeric_index_to_remove: int, index (0–6) of the numeric feature to remove\n",
    "      (0 = first numeric feature, corresponds to 'Exact Mass', etc.)\n",
    "\n",
    "    Returns:\n",
    "    - modified_graphs: list of modified PyG Data objects\n",
    "    \"\"\"\n",
    "    modified_graphs = []\n",
    "    for g in graphs:\n",
    "        new_graph = g.clone()\n",
    "        # Split graph features\n",
    "        morgan_fp = new_graph.graph_features[:2048]\n",
    "        numeric_feats = new_graph.graph_features[2048:]\n",
    "\n",
    "        # Remove one numeric feature\n",
    "        mask = torch.ones(numeric_feats.size(0), dtype=torch.bool)\n",
    "        mask[numeric_index_to_remove] = False\n",
    "        reduced_numeric_feats = numeric_feats[mask]\n",
    "\n",
    "        # Combine back\n",
    "        new_graph.graph_features = torch.cat([morgan_fp, reduced_numeric_feats], dim=0)\n",
    "        modified_graphs.append(new_graph)\n",
    "    return modified_graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FL58blL_xGs9",
    "outputId": "9be935b5-77c6-466e-8cd2-5ae090a5f989"
   },
   "outputs": [],
   "source": [
    "for i, feature_name in enumerate(numeric_columns):\n",
    "    print(f\"\\n>>> Removing feature: {feature_name}\")\n",
    "    ablated_graphs = remove_numeric_feature(graphs, i)\n",
    "    # Then rerun training/eval using `ablated_graphs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XBmvfP_uxZSp",
    "outputId": "5fed656c-1a3e-41d5-d383-8d9aaa54a4b6"
   },
   "outputs": [],
   "source": [
    "# Step 1: Remove the numeric feature (e.g., index 2 = 'Ring Count')\n",
    "ablated_graphs = remove_numeric_feature(graphs, 0)\n",
    "\n",
    "# Step 2: Train/test split (80/20)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_indices, test_indices = train_test_split(\n",
    "    list(range(len(ablated_graphs))), test_size=0.2, random_state=42\n",
    ")\n",
    "train_graphs = [ablated_graphs[i] for i in train_indices]\n",
    "test_graphs = [ablated_graphs[i] for i in test_indices]\n",
    "\n",
    "# Step 3: Create DataLoaders\n",
    "train_loader = DataLoader(train_graphs, batch_size=4, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_graphs, batch_size=4, shuffle=False, drop_last=True)\n",
    "\n",
    "# Step 4: Re-initialize model\n",
    "num_node_features = ablated_graphs[0].num_node_features\n",
    "num_edge_features = ablated_graphs[0].edge_attr.size(1) if ablated_graphs[0].edge_attr is not None else 0\n",
    "num_graph_features = ablated_graphs[0].graph_features.size(0)\n",
    "\n",
    "model = GATWithGraphFeatures(\n",
    "    num_node_features=num_node_features,\n",
    "    num_edge_features=num_edge_features,\n",
    "    num_graph_features=num_graph_features,\n",
    "    hidden_channels=64,\n",
    "    num_classes=3\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 5: Training loop\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = F.nll_loss(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Step 6: Evaluate on test set\n",
    "accuracy, precision, recall = evaluate_more(model, test_loader)\n",
    "print(\"\\n=== Test Performance ===\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bs4yWkVnbDb-",
    "outputId": "a15fa3fa-6583-4f95-f1ce-077de86e34b5"
   },
   "outputs": [],
   "source": [
    "mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048)\n",
    "\n",
    "# Function to compute Morgan Fingerprints\n",
    "def compute_morgan_fingerprint(smiles, radius=2, fpSize=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        # Generate Morgan fingerprint\n",
    "        fingerprint = mfpgen.GetFingerprint(mol)\n",
    "        return np.array(fingerprint)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Have checked, no none\n",
    "df['morgan_fingerprint'] = df['SMILES'].apply(compute_morgan_fingerprint)\n",
    "\n",
    "numeric_columns = ['Exact Mass', 'XLogP3', 'Ring Count',\n",
    "                   'Hydrogen Bond Acceptor Count', 'Hydrogen Bond Donor Count',\n",
    "                   'Rotatable Bond Count', 'Topological Polar Surface Area']\n",
    "\n",
    "# Additional columns to include (if any)\n",
    "additional_columns = []\n",
    "data = mol.drop_duplicates(subset = ['Pubid'], keep='first')\n",
    "new_df = pd.merge(data, df, left_on='Smiles', right_on='SMILES')\n",
    "# Combine the selected columns with the \"Features\" (fingerprints) to form the input data (X)\n",
    "df_combined = new_df[numeric_columns + additional_columns].copy()\n",
    "df_combined = df_combined.astype(float)\n",
    "\n",
    "df_full = pd.concat([df, df_combined], axis=1, ignore_index=False)\n",
    "\n",
    "graphs = []\n",
    "\n",
    "for i, graph_data in enumerate(featurized_data):\n",
    "    # Node-level features\n",
    "    node_features = torch.tensor(graph_data.node_features, dtype=torch.float)  # Original node features\n",
    "\n",
    "    # Graph-level features\n",
    "    morgan_fp = df_full.loc[i, 'morgan_fingerprint']  # Extract Morgan fingerprint\n",
    "    if isinstance(morgan_fp, np.ndarray):\n",
    "        morgan_fp = torch.tensor(morgan_fp.astype(np.float32), dtype=torch.float)\n",
    "    else:\n",
    "        raise TypeError(f\"Expected np.ndarray for 'morgan_fingerprint', got {type(morgan_fp)}\")\n",
    "\n",
    "    # Extract additional graph-level features from df_full\n",
    "    additional_features = torch.tensor(df_full.loc[i, numeric_columns], dtype=torch.float)\n",
    "\n",
    "    # Combine all graph-level features into a single tensor\n",
    "    graph_features = torch.cat([morgan_fp, additional_features], dim=0)  # Shape: [2048 + len(additional_features_list)]\n",
    "\n",
    "    # Edge indices and attributes\n",
    "    edge_index = torch.tensor(graph_data.edge_index, dtype=torch.long)\n",
    "    edge_attr = torch.tensor(graph_data.edge_features, dtype=torch.float)\n",
    "\n",
    "    # Target label\n",
    "    y = torch.tensor(df_full['count_binned_custom'][i], dtype=torch.long)\n",
    "\n",
    "    # Create a PyTorch Geometric Data object\n",
    "    data = Data(\n",
    "        x=node_features,              # Node-level features\n",
    "        edge_index=edge_index,        # Edge indices\n",
    "        edge_attr=edge_attr,          # Edge features\n",
    "        y=y,                          # Target label\n",
    "        graph_features=graph_features  # Graph-level features\n",
    "    )\n",
    "    print(data)\n",
    "    graphs.append(data)\n",
    "\n",
    "# Create a DataLoader\n",
    "loader = DataLoader(graphs, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_edge_features, hidden_channels, num_classes):\n",
    "        super(GAT, self).__init__()\n",
    "        # Use GATConv which supports edge features\n",
    "        self.conv1 = GATConv(num_node_features, hidden_channels, edge_dim=num_edge_features)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels, edge_dim=num_edge_features)\n",
    "        self.lin = torch.nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # First graph convolution layer (with edge attributes)\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Second graph convolution layer (with edge attributes)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Apply global mean pooling to aggregate node features\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Apply dropout for regularization\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        # Linear layer to get final logits\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # LogSoftmax for multi-class classification\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "class GATWithGraphFeatures(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_edge_features, num_graph_features, hidden_channels, num_classes):\n",
    "        super(GATWithGraphFeatures, self).__init__()\n",
    "\n",
    "        # GAT layers for node features\n",
    "        self.conv1 = GATConv(num_node_features, hidden_channels, edge_dim=num_edge_features)\n",
    "        self.conv2 = GATConv(hidden_channels , hidden_channels, edge_dim=num_edge_features)\n",
    "\n",
    "        # Linear layers for node feature processing after pooling\n",
    "        self.node_lin1 = nn.Linear(hidden_channels, hidden_channels)\n",
    "\n",
    "        # Linear layers for graph-level features\n",
    "        self.graph_lin1 = nn.Linear(num_graph_features * 43, hidden_channels)\n",
    "        self.graph_lin2 = nn.Linear(hidden_channels, hidden_channels)\n",
    "\n",
    "        # Final linear layer combining node and graph features\n",
    "        self.final_lin = nn.Linear(hidden_channels * 2, num_classes)  # Combine node and graph embeddings\n",
    "\n",
    "        # Regularization layers\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.batch_norm_graph = nn.BatchNorm1d(hidden_channels)  # For graph features\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        graph_features = data.graph_features  # Graph-level features\n",
    "\n",
    "        # First graph convolution layer (with edge attributes)\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm1(x)\n",
    "\n",
    "        # Second graph convolution layer (with edge attributes)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm2(x)\n",
    "\n",
    "        # Global pooling to get graph-level representation from node features\n",
    "        node_embedding = global_max_pool(x, batch)  # Shape: [batch_size, hidden_channels * num_heads]\n",
    "        node_embedding = self.node_lin1(node_embedding)\n",
    "        node_embedding = F.relu(node_embedding)\n",
    "        node_embedding = self.dropout(node_embedding)\n",
    "\n",
    "        # Process graph-level features\n",
    "        graph_embedding = self.graph_lin1(graph_features)\n",
    "        graph_embedding = F.relu(graph_embedding)\n",
    "        graph_embedding = self.graph_lin2(graph_embedding)\n",
    "        graph_embedding = F.relu(graph_embedding)\n",
    "        graph_embedding = self.dropout(graph_embedding)\n",
    "\n",
    "        # Combine node and graph embeddings\n",
    "        graph_embedding_expanded = graph_embedding.unsqueeze(0).expand(node_embedding.size(0), -1)  # [32, 64]\n",
    "        combined = torch.cat([node_embedding, graph_embedding_expanded], dim=-1)  # Shape: [batch_size, hidden_channels * 2]\n",
    "\n",
    "        # Debugging shape before final linear layer\n",
    "\n",
    "        # Final classification layer\n",
    "        out = self.final_lin(combined)  # This should match the expected size\n",
    "        return F.log_softmax(out, dim=-1)\n",
    "\n",
    "\n",
    "# Assuming 'graphs' is already created with node and graph-level features\n",
    "num_graphs = 172  # Adjust based on dataset size\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = 129\n",
    "test_size = num_graphs - train_size\n",
    "train_graphs, test_graphs = torch.utils.data.random_split(\n",
    "    graphs,\n",
    "    [train_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # Fixed seed for splits\n",
    ")\n",
    "\n",
    "# Create DataLoaders with consistent shuffling\n",
    "train_loader = DataLoader(\n",
    "    train_graphs,\n",
    "    batch_size=43,\n",
    "    shuffle=True,\n",
    "    generator=torch.Generator().manual_seed(42)  # Fixed shuffle seed\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_graphs,\n",
    "    batch_size=43,\n",
    "    shuffle=False  # Never shuffle test set!\n",
    ")\n",
    "\n",
    "# Get feature dimensions\n",
    "num_node_features = graphs[0].num_node_features\n",
    "num_edge_features = graphs[0].edge_attr.size(1) if graphs[0].edge_attr is not None else 0\n",
    "num_graph_features = graphs[0].graph_features.size(0)  # From Morgan FP + additional features (e.g., 2056)\n",
    "# num_graph_features = 0\n",
    "# Initialize the model\n",
    "model = GATWithGraphFeatures(\n",
    "    num_node_features=num_node_features,\n",
    "    num_edge_features=num_edge_features,\n",
    "    num_graph_features=num_graph_features,\n",
    "    hidden_channels=64,\n",
    "    num_classes=3\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Function to evaluate the model (simple accuracy)\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            out = model(batch)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == batch.y).sum().item()\n",
    "            total += batch.y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Function to evaluate the model with more metrics\n",
    "def evaluate_more(model, loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            out = model(batch)\n",
    "            pred = out.argmax(dim=1)\n",
    "            y_true.extend(batch.y.cpu().numpy())\n",
    "            y_pred.extend(pred.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    return accuracy, precision, recall\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = F.nll_loss(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Print the loss for the current epoch\n",
    "    print(f\"Epoch {epoch+1}/100, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluate on the training set (you can change this to test_loader if preferred)\n",
    "    val_accuracy = evaluate(model, train_loader)  # Changed from 'loader' to 'train_loader'\n",
    "    print(f\"Validation Accuracy after Epoch {epoch+1}: {val_accuracy:.4f}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "test_loader = DataLoader(test_graphs, batch_size=43, shuffle=True, drop_last = True)\n",
    "test_accuracy, test_precision, test_recall = evaluate_more(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
