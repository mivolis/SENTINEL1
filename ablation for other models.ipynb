{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b36yjIKI82KB",
    "outputId": "e4438fff-4255-4574-aec8-f67bb10fc823"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mol = pd.read_csv(\"/content/drive/My Drive/result.csv\")\n",
    "\n",
    "smiles = mol.drop_duplicates(subset = ['Pubid'], keep='first')\n",
    "smiles = smiles['Smiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "llHBOMb461bo",
    "outputId": "45d3b5d3-5b25-4603-a9b5-b0c0558746bd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from deepchem.feat import PagtnMolGraphFeaturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X2g5GPgd9CGC",
    "outputId": "6e658008-3a0c-416b-ef3a-9ef200edbed0"
   },
   "outputs": [],
   "source": [
    "from deepchem.feat import PagtnMolGraphFeaturizer\n",
    "\n",
    "# Initialize the featurizer\n",
    "featurizer = PagtnMolGraphFeaturizer(max_length=5)\n",
    "\n",
    "# Apply the featurizer to the SMILES series\n",
    "featurized_data = featurizer.featurize(smiles.tolist())\n",
    "\n",
    "from deepchem.feat import PagtnMolGraphFeaturizer\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the featurizer\n",
    "featurizer = PagtnMolGraphFeaturizer(max_length=5)\n",
    "\n",
    "# Apply the featurizer to the SMILES series\n",
    "featurized_data = featurizer.featurize(smiles.tolist())\n",
    "\n",
    "graphs = []\n",
    "\n",
    "for i, graph_data in enumerate(featurized_data):\n",
    "    # Convert node features and edge indices into PyTorch tensors\n",
    "    x = torch.tensor(graph_data.node_features, dtype=torch.float)  # Node features\n",
    "    edge_index = torch.tensor(graph_data.edge_index, dtype=torch.long).t().contiguous()  # Edge index\n",
    "\n",
    "    # Create a PyTorch Geometric Data object\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "    graphs.append(data)\n",
    "\n",
    "# Create a DataLoader\n",
    "loader = DataLoader(graphs, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntUS2Z7eVvOh",
    "outputId": "0de48999-7d1a-4ff3-b8ce-d13d00988e54"
   },
   "outputs": [],
   "source": [
    "mol = pd.read_csv(\"/content/drive/My Drive/result.csv\")\n",
    "\n",
    "smiles = mol.drop_duplicates(subset = ['Pubid'], keep='first')\n",
    "smiles = smiles['Smiles']\n",
    "\n",
    "smiles_counts = mol['Smiles'].value_counts().reset_index()\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "# Initialize the featurizer\n",
    "featurizer = PagtnMolGraphFeaturizer(max_length=5)\n",
    "\n",
    "# Apply the featurizer to the SMILES series\n",
    "featurized_data = featurizer.featurize(smiles.tolist())\n",
    "\n",
    "# Initialize the featurizer\n",
    "featurizer = PagtnMolGraphFeaturizer(max_length=5)\n",
    "\n",
    "# Apply the featurizer to the SMILES series\n",
    "featurized_data = featurizer.featurize(smiles.tolist())\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, graph_data in enumerate(featurized_data):\n",
    "    result = {\n",
    "        \"SMILES\": smiles.iloc[i],\n",
    "        \"Node Features\": graph_data.node_features.tolist(),  # Convert to list for storage\n",
    "        \"Edge Features\": graph_data.edge_features.tolist(),   # Convert to list for storage\n",
    "        \"Edge Index\": graph_data.edge_index.tolist()         # Convert to list for storage\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df.to_csv(\"node_edge_features.csv\", index=False)\n",
    "\n",
    "# Display the results\n",
    "print(results_df.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxvVQps49TBV"
   },
   "outputs": [],
   "source": [
    "smiles_counts = mol['Smiles'].value_counts().reset_index()\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "df = pd.merge(results_df, smiles_counts, left_on='SMILES', right_on='Smiles', how = 'left')\n",
    "(df['count']).describe()\n",
    "\n",
    "unique_interactions = mol.groupby(\"Compound ID\")[\"gene_name\"].nunique()\n",
    "\n",
    "# Display results\n",
    "df_dedup = unique_interactions.reset_index()\n",
    "df['number_dedup'] = df_dedup['gene_name']\n",
    "df['count_binned_custom'] = pd.cut(df['number_dedup'], bins=[0, 16, 60.1, 115], labels=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iaCAxxpK9vLj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def aggregate_features1(node_features, edge_features):\n",
    "    # Node statistics\n",
    "    node_mean = np.mean(node_features, axis=0)\n",
    "    node_max = np.max(node_features, axis=0)\n",
    "    node_min = np.min(node_features, axis=0)\n",
    "    node_median = np.median(node_features, axis=0)\n",
    "    node_std = np.std(node_features, axis=0)\n",
    "    node_skew = skew(node_features, axis=0)\n",
    "    node_kurtosis = kurtosis(node_features, axis=0)\n",
    "\n",
    "    # Edge statistics\n",
    "    edge_mean = np.mean(edge_features, axis=0)\n",
    "    edge_max = np.max(edge_features, axis=0)\n",
    "    edge_min = np.min(edge_features, axis=0)\n",
    "    edge_median = np.median(edge_features, axis=0)\n",
    "    edge_std = np.std(edge_features, axis=0)\n",
    "    edge_skew = skew(edge_features, axis=0)\n",
    "    edge_kurtosis = kurtosis(edge_features, axis=0)\n",
    "\n",
    "    return np.concatenate([\n",
    "        node_mean, node_max, node_min, node_median, node_std, node_skew, node_kurtosis,\n",
    "        edge_mean, edge_max, edge_min, edge_median, edge_std, edge_skew, edge_kurtosis\n",
    "    ])\n",
    "\n",
    "def aggregate_features(node_features, edge_features):\n",
    "    node_mean = np.mean(node_features, axis=0)\n",
    "    node_max = np.max(node_features, axis=0)\n",
    "    node_std = np.std(node_features, axis=0)\n",
    "\n",
    "    edge_mean = np.mean(edge_features, axis=0)\n",
    "    edge_max = np.max(edge_features, axis=0)\n",
    "    edge_std = np.std(edge_features, axis=0)\n",
    "\n",
    "    return np.concatenate([node_mean, node_max, node_std, edge_mean, edge_max, edge_std])\n",
    "\n",
    "# Apply aggregation to all molecules\n",
    "df[\"Features\"] = df.apply(lambda row: aggregate_features(row[\"Node Features\"], row[\"Edge Features\"]), axis=1)\n",
    "X = np.vstack(df[\"Features\"].values)\n",
    "\n",
    "# Convert list of features into a structured DataFrame\n",
    "X = np.vstack(df[\"Features\"].values)  # Shape: (num_samples, num_features)\n",
    "y = df[\"count_binned_custom\"].values  # Target labels\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DQkL7gZ0BvKL",
    "outputId": "a2ca1096-b940-4e88-c92c-87183ca75567"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OLLDvxa5_ROa",
    "outputId": "eaf39ba2-ed87-4e4a-d812-48921c3076df"
   },
   "outputs": [],
   "source": [
    "# Shuffle dataset\n",
    "df = shuffle(df, random_state=42)\n",
    "\n",
    "# Define Random Forest model\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=100,  # More trees\n",
    "    max_depth=200,      # Deeper trees\n",
    "    min_samples_split=5,  # Reduce overfitting\n",
    "    class_weight=\"balanced\",  # Handle class imbalance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "scores = cross_val_score(clf, X, y, cv=10, scoring=\"accuracy\")\n",
    "\n",
    "# Print results\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean accuracy:\", scores.mean())\n",
    "print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "XsBtQa4LBdoR",
    "outputId": "a0bd685a-ee38-4ab3-b29e-8aa18f1e6981"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train Random Forest\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "# # Sort feature importances in descending order\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.bar(range(X.shape[1]), importances, align=\"center\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.show()\n",
    "\n",
    "node_max_importance = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 668
    },
    "id": "jD1Zh2FhQNlr",
    "outputId": "6229fe70-f1b0-44c4-f0ce-e707f8b2cb75"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Example: Suppose you already have a trained RandomForest\n",
    "# and have obtained feature_importances_ of shape (408,).\n",
    "# Replace the line below with your actual importances.\n",
    "# -----------------------------------------------------------\n",
    "feature_importances = importances  # Simulated importances\n",
    "\n",
    "# Define constants for slicing\n",
    "NUM_NODE_FEATURES = 94  # node_mean, node_max, node_min each has 94 features\n",
    "NUM_EDGE_FEATURES = 42  # edge_mean, edge_max, edge_min each has 42 features\n",
    "\n",
    "# Slicing indices for each group\n",
    "node_mean_slice = feature_importances[0 : NUM_NODE_FEATURES]\n",
    "node_max_slice  = feature_importances[NUM_NODE_FEATURES : 2*NUM_NODE_FEATURES]\n",
    "node_std_slice  = feature_importances[2*NUM_NODE_FEATURES : 3*NUM_NODE_FEATURES]\n",
    "\n",
    "edge_mean_slice = feature_importances[3*NUM_NODE_FEATURES : 3*NUM_NODE_FEATURES + NUM_EDGE_FEATURES]\n",
    "edge_max_slice  = feature_importances[3*NUM_NODE_FEATURES + NUM_EDGE_FEATURES : 3*NUM_NODE_FEATURES + 2*NUM_EDGE_FEATURES]\n",
    "edge_std_slice  = feature_importances[3*NUM_NODE_FEATURES + 2*NUM_EDGE_FEATURES : 3*NUM_NODE_FEATURES + 3*NUM_EDGE_FEATURES]\n",
    "\n",
    "# Sum the importances within each group\n",
    "node_mean_imp = node_mean_slice.sum()\n",
    "node_max_imp  = node_max_slice.sum()\n",
    "node_std_imp  = node_std_slice.sum()\n",
    "edge_mean_imp = edge_mean_slice.sum()\n",
    "edge_max_imp  = edge_max_slice.sum()\n",
    "edge_std_imp  = edge_std_slice.sum()\n",
    "\n",
    "# Prepare data for plotting\n",
    "groups = [\"node_mean\", \"node_max\", \"node_std\", \"edge_mean\", \"edge_max\", \"edge_std\"]\n",
    "importances_aggregated = [\n",
    "    node_mean_imp,\n",
    "    node_max_imp,\n",
    "    node_std_imp,\n",
    "    edge_mean_imp,\n",
    "    edge_max_imp,\n",
    "    edge_std_imp\n",
    "]\n",
    "\n",
    "# Plot the aggregated feature importances\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(groups, importances_aggregated, color=[\"#4C72B0\", \"#55A868\", \"#C44E52\", \"#8172B2\", \"#CCB974\", \"#64B5CD\"])\n",
    "plt.xlabel(\"Feature Group\")\n",
    "plt.ylabel(\"Aggregated Feature Importance\")\n",
    "plt.title(\"Aggregated Feature Importance by Group\")\n",
    "plt.show()\n",
    "\n",
    "# Print numeric values\n",
    "for grp, val in zip(groups, importances_aggregated):\n",
    "    print(f\"{grp}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 668
    },
    "id": "7TnjoQFVY5BX",
    "outputId": "d1bb2639-8ce2-44b6-91e3-d3eafda4a277"
   },
   "outputs": [],
   "source": [
    "groups = [\"node_mean\", \"node_max\", \"node_std\", \"edge_mean\", \"edge_max\", \"edge_std\"]\n",
    "importances_aggregated = [\n",
    "    node_mean_imp/94,\n",
    "    node_max_imp/94,\n",
    "    node_std_imp/94,\n",
    "    edge_mean_imp/42,\n",
    "    edge_max_imp/42,\n",
    "    edge_std_imp/42\n",
    "]\n",
    "\n",
    "# Plot the aggregated feature importances\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(groups, importances_aggregated, color=[\"#4C72B0\", \"#55A868\", \"#C44E52\", \"#8172B2\", \"#CCB974\", \"#64B5CD\"])\n",
    "plt.xlabel(\"Feature Group\")\n",
    "plt.ylabel(\"Mean Aggregated Feature Importance\")\n",
    "plt.title(\"Mean Aggregated Feature Importance by Group\")\n",
    "plt.show()\n",
    "\n",
    "# Print numeric values\n",
    "for grp, val in zip(groups, importances_aggregated):\n",
    "    print(f\"{grp}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99f4yJpFZSWa"
   },
   "source": [
    "## Secondly more features to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZLkJIfDvZcRl",
    "outputId": "1c06f27f-4ab2-434e-e8c8-e6508662d864"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def aggregate_features1(node_features, edge_features):\n",
    "    # Node statistics\n",
    "    node_mean = np.mean(node_features, axis=0)\n",
    "    node_max = np.max(node_features, axis=0)\n",
    "    node_min = np.min(node_features, axis=0)\n",
    "    node_median = np.median(node_features, axis=0)\n",
    "    node_std = np.std(node_features, axis=0)\n",
    "    node_skew = skew(node_features, axis=0)\n",
    "    node_kurtosis = kurtosis(node_features, axis=0)\n",
    "\n",
    "    # Edge statistics\n",
    "    edge_mean = np.mean(edge_features, axis=0)\n",
    "    edge_max = np.max(edge_features, axis=0)\n",
    "    edge_min = np.min(edge_features, axis=0)\n",
    "    edge_median = np.median(edge_features, axis=0)\n",
    "    edge_std = np.std(edge_features, axis=0)\n",
    "    edge_skew = skew(edge_features, axis=0)\n",
    "    edge_kurtosis = kurtosis(edge_features, axis=0)\n",
    "\n",
    "    return np.concatenate([\n",
    "        node_mean, node_max, node_min, node_median, node_std, node_skew, node_kurtosis,\n",
    "        edge_mean, edge_max, edge_min, edge_median, edge_std, edge_skew, edge_kurtosis\n",
    "    ])\n",
    "\n",
    "# Apply aggregation to all molecules\n",
    "df[\"Features1\"] = df.apply(lambda row: aggregate_features1(row[\"Node Features\"], row[\"Edge Features\"]), axis=1)\n",
    "X = np.vstack(df[\"Features1\"].values)\n",
    "\n",
    "# Convert list of features into a structured DataFrame\n",
    "X = np.vstack(df[\"Features1\"].values)  # Shape: (num_samples, num_features)\n",
    "y = df[\"count_binned_custom\"].values  # Target labels\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nA8wXaG5ZhYz",
    "outputId": "441b0be1-4e46-49a7-fc10-5e4217154a39"
   },
   "outputs": [],
   "source": [
    "# Shuffle dataset\n",
    "df = shuffle(df, random_state=42)\n",
    "\n",
    "# Define Random Forest model\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=100,  # More trees\n",
    "    max_depth=200,      # Deeper trees\n",
    "    min_samples_split=5,  # Reduce overfitting\n",
    "    class_weight=\"balanced\",  # Handle class imbalance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "scores = cross_val_score(clf, X, y, cv=10, scoring=\"accuracy\")\n",
    "\n",
    "# Print results\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean accuracy:\", scores.mean())\n",
    "print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "# # Sort feature importances in descending order\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.bar(range(X.shape[1]), importances, align=\"center\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.show()\n",
    "\n",
    "feature_importances = importances  # Simulated importances\n",
    "\n",
    "# Define constants for slicing\n",
    "NUM_NODE_FEATURES = 94  # node_mean, node_max, node_min each has 94 features\n",
    "NUM_EDGE_FEATURES = 42  # edge_mean, edge_max, edge_min each has 42 features\n",
    "\n",
    "# Slicing indices for each group\n",
    "node_mean_slice = feature_importances[0 : NUM_NODE_FEATURES]\n",
    "node_max_slice  = feature_importances[NUM_NODE_FEATURES : 2*NUM_NODE_FEATURES]\n",
    "node_min_slice  = feature_importances[2*NUM_NODE_FEATURES : 3*NUM_NODE_FEATURES]\n",
    "node_median_slice = feature_importances[3*NUM_NODE_FEATURES : 4*NUM_NODE_FEATURES]\n",
    "node_std_slice  = feature_importances[4*NUM_NODE_FEATURES : 5*NUM_NODE_FEATURES]\n",
    "node_skew_slice = feature_importances[5*NUM_NODE_FEATURES : 6*NUM_NODE_FEATURES]\n",
    "node_kurtosis_slice = feature_importances[6*NUM_NODE_FEATURES : 7*NUM_NODE_FEATURES]\n",
    "\n",
    "edge_mean_slice = feature_importances[7*NUM_NODE_FEATURES : 7*NUM_NODE_FEATURES + NUM_EDGE_FEATURES]\n",
    "edge_max_slice  = feature_importances[7*NUM_NODE_FEATURES + NUM_EDGE_FEATURES : 7*NUM_NODE_FEATURES + 2*NUM_EDGE_FEATURES]\n",
    "edge_min_slice  = feature_importances[7*NUM_NODE_FEATURES + 2*NUM_EDGE_FEATURES : 7*NUM_NODE_FEATURES + 3*NUM_EDGE_FEATURES]\n",
    "edge_median_slice = feature_importances[7*NUM_NODE_FEATURES + 3*NUM_EDGE_FEATURES : 7*NUM_NODE_FEATURES + 4*NUM_EDGE_FEATURES]\n",
    "edge_std_slice  = feature_importances[7*NUM_NODE_FEATURES + 4*NUM_EDGE_FEATURES : 7*NUM_NODE_FEATURES + 5*NUM_EDGE_FEATURES]\n",
    "edge_skew_slice = feature_importances[7*NUM_NODE_FEATURES + 5*NUM_EDGE_FEATURES : 7*NUM_NODE_FEATURES + 6*NUM_EDGE_FEATURES]\n",
    "edge_kurtosis_slice = feature_importances[7*NUM_NODE_FEATURES + 6*NUM_EDGE_FEATURES : 7*NUM_NODE_FEATURES + 7*NUM_EDGE_FEATURES]\n",
    "\n",
    "# Sum the importances within each group\n",
    "node_mean_imp = node_mean_slice.sum()\n",
    "node_max_imp  = node_max_slice.sum()\n",
    "node_min_imp  = node_min_slice.sum()\n",
    "node_median_imp = node_median_slice.sum()\n",
    "node_std_imp  = node_std_slice.sum()\n",
    "node_skew_imp = node_skew_slice.sum()\n",
    "node_kurtosis_imp = node_kurtosis_slice.sum()\n",
    "\n",
    "edge_mean_imp = edge_mean_slice.sum()\n",
    "edge_max_imp  = edge_max_slice.sum()\n",
    "edge_min_imp  = edge_min_slice.sum()\n",
    "edge_median_imp = edge_median_slice.sum()\n",
    "edge_std_imp  = edge_std_slice.sum()\n",
    "edge_skew_imp = edge_skew_slice.sum()\n",
    "edge_kurtosis_imp = edge_kurtosis_slice.sum()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "groups = [\"node_mean\", \"node_max\", \"node_min\", \"node_median\", \"node_std\", \"node_skew\", \"node_kurtosis\", \"edge_mean\", \"edge_max\", \"edge_min\", \"edge_median\", \"edge_std\", \"edge_skew\", \"edge_kurtosis\"]\n",
    "importances_aggregated = [\n",
    "    node_mean_imp/94,\n",
    "    node_max_imp/94,\n",
    "    node_min_imp/94,\n",
    "    node_median_imp/94,\n",
    "    node_std_imp/94,\n",
    "    node_skew_imp/94,\n",
    "    node_kurtosis_imp/94,\n",
    "    edge_mean_imp/42,\n",
    "    edge_max_imp/42,\n",
    "    edge_min_imp/42,\n",
    "    edge_median_imp/42,\n",
    "    edge_std_imp/42,\n",
    "    edge_skew_imp/42,\n",
    "    edge_kurtosis_imp/42\n",
    "]\n",
    "\n",
    "# Plot the aggregated feature importances\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(groups, importances_aggregated)\n",
    "plt.xlabel(\"Feature Group\")\n",
    "plt.ylabel(\"Mean Aggregated Feature Importance\")\n",
    "plt.title(\"Mean Aggregated Feature Importance by Group\")\n",
    "\n",
    "# Rotate the x-labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print numeric values\n",
    "for grp, val in zip(groups, importances_aggregated):\n",
    "    print(f\"{grp}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-zK9-vpcHIR"
   },
   "source": [
    "##Final decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XrEKOTYkcF7z",
    "outputId": "d08b5c62-8e85-42b2-f0ee-3af3310f872e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def aggregate_features1(node_features, edge_features):\n",
    "    # Node statistics\n",
    "    node_mean = np.mean(node_features, axis=0)\n",
    "    node_std = np.std(node_features, axis=0)\n",
    "    node_skew = skew(node_features, axis=0)\n",
    "    node_kurtosis = kurtosis(node_features, axis=0)\n",
    "    # node_max = np.max(node_features, axis=0)\n",
    "\n",
    "    # Edge statistics\n",
    "    edge_mean = np.mean(edge_features, axis=0)\n",
    "    edge_std = np.std(edge_features, axis=0)\n",
    "    edge_skew = skew(edge_features, axis=0)\n",
    "    edge_kurtosis = kurtosis(edge_features, axis=0)\n",
    "    # edge_max = np.max(edge_features, axis=0)\n",
    "\n",
    "    return np.concatenate([\n",
    "        node_mean,\n",
    "        node_std,\n",
    "        node_skew,\n",
    "        node_kurtosis,\n",
    "        edge_mean,\n",
    "        edge_std,\n",
    "        edge_skew,\n",
    "        edge_kurtosis\n",
    "    ])\n",
    "\n",
    "# Apply aggregation to all molecules\n",
    "df[\"Features5\"] = df.apply(lambda row: aggregate_features1(row[\"Node Features\"], row[\"Edge Features\"]), axis=1)\n",
    "X = np.vstack(df[\"Features5\"].values)\n",
    "\n",
    "# Convert list of features into a structured DataFrame\n",
    "X = np.vstack(df[\"Features5\"].values)  # Shape: (num_samples, num_features)\n",
    "y = df[\"count_binned_custom\"].values  # Target labels\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ida8al8kc7Ko",
    "outputId": "bb2f70f2-1551-4a60-a877-72bd1a79309e"
   },
   "outputs": [],
   "source": [
    "X = np.vstack(df[\"Features5\"].values)  # Shape: (num_samples, num_features)\n",
    "df = shuffle(df, random_state=42)\n",
    "df['count_binned_custom'] = pd.cut(df['number_dedup'], bins=[0, 18, 60, 115], labels=[0, 1, 2])\n",
    "y = df['count_binned_custom']\n",
    "\n",
    "# Define Random Forest model\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=100,  # More trees\n",
    "    max_depth=200,      # Deeper trees\n",
    "    min_samples_split=5,  # Reduce overfitting\n",
    "    class_weight=\"balanced\",  # Handle class imbalance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "scores = cross_val_score(clf, X, y, cv=10, scoring=\"accuracy\")\n",
    "\n",
    "# Print results\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean accuracy:\", scores.mean())\n",
    "print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_hKDEKTlOoH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Shuffle and clean data\n",
    "df = shuffle(df, random_state=42)\n",
    "df['count_binned_custom'] = pd.cut(df['number_dedup'], bins=[0, 16, 60.1, 115], labels=[0, 1, 2]).astype(int)\n",
    "\n",
    "y = df[\"count_binned_custom\"].values\n",
    "\n",
    "numeric_columns = ['Exact Mass', 'XLogP3', 'Heavy Atom Count', 'Ring Count',\n",
    "                   'Hydrogen Bond Acceptor Count', 'Hydrogen Bond Donor Count',\n",
    "                   'Rotatable Bond Count', 'Topological Polar Surface Area']\n",
    "\n",
    "additional_columns = []\n",
    "\n",
    "# Combine the selected columns with the \"Features\" (fingerprints) to form the input data (X)\n",
    "df_combined = new_df[numeric_columns + additional_columns].copy()\n",
    "df_combined = df_combined.astype(float)\n",
    "\n",
    "# Ensure that 'Features' contains the fingerprint data and combine it\n",
    "X = np.vstack(df[\"Features5\"].values)  # Features from RDKit or other representation\n",
    "X_combined = np.hstack((df_combined.values, X))  # Combine the features\n",
    "\n",
    "X_all = np.hstack((X_combined, fingerprints))\n",
    "X_all = np.nan_to_num(X_all)\n",
    "# Models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500, random_state=42),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Accuracy evaluation\n",
    "def compute_cv_accuracy(X, y, clf):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    X_clean = np.nan_to_num(X)  # Ensure no NaNs\n",
    "    return np.mean(cross_val_score(clf, X, y, cv=skf, scoring='accuracy'))\n",
    "\n",
    "def compute_cv_auc(X, y_bin, clf):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    y_proba = cross_val_predict(clf, X, y, cv=skf, method='predict_proba')\n",
    "    return roc_auc_score(y_bin, y_proba, average=\"macro\", multi_class=\"ovr\")\n",
    "\n",
    "# Feature index helpers\n",
    "fp_len = len(fingerprints[0])\n",
    "node_dim = 94 * 4\n",
    "edge_dim = 42 * 4\n",
    "\n",
    "# Run ablation per model\n",
    "for model_name, clf in models.items():\n",
    "    acc_values = []\n",
    "\n",
    "    # Baseline\n",
    "    acc_values.append(compute_cv_accuracy(X_all, y, clf))\n",
    "\n",
    "    # Remove Fingerprints\n",
    "    X_no_fp = np.delete(X_all, np.s_[-fp_len:], axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_no_fp, y, clf))\n",
    "\n",
    "    # Remove Exact Mass\n",
    "    X_ablated = np.delete(X_no_fp, 0, axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_ablated, y, clf))\n",
    "\n",
    "    # Remove XLogP3\n",
    "    X_ablated = np.delete(X_ablated, 0, axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_ablated, y, clf))\n",
    "\n",
    "    # Remove Ring Count (index 3 in your numeric list)\n",
    "    X_ablated = np.delete(X_ablated, 1, axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_ablated, y, clf))\n",
    "\n",
    "    # Remove PAGTN Edge Features\n",
    "    X_ablated = np.delete(X_ablated, np.s_[- edge_dim:], axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_ablated, y, clf))\n",
    "\n",
    "    # Remove PAGTN Node Features\n",
    "    X_ablated = np.delete(X_ablated, np.s_[-node_dim:], axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_ablated, y, clf))\n",
    "\n",
    "    # Remove Hydrogen Bond Acceptor Count (index 4)\n",
    "    X_ablated = np.delete(X_ablated, 1, axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_ablated, y, clf))\n",
    "\n",
    "    # Remove Hydrogen Bond Donor Count (index 5)\n",
    "    X_ablated = np.delete(X_ablated, 1, axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_ablated, y, clf))\n",
    "\n",
    "    # Remove Rotatable Bonds (index 6)\n",
    "    X_ablated = np.delete(X_ablated, 1, axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_ablated, y, clf))\n",
    "\n",
    "    # Labels in specified order\n",
    "    labels = [\n",
    "        \"Baseline\",\n",
    "        \"Remove Fingerprint\",\n",
    "        \"Remove Exact Mass\",\n",
    "        \"Remove XLogP3\",\n",
    "        \"Remove Ring Count\",\n",
    "        \"Remove PAGTN Edge Feature\",\n",
    "        \"Remove PAGTN Node Feature\",\n",
    "        'Remove Hydrogen Bond Acceptor Count',\n",
    "        'Remove Hydrogen Bond Donor Count',\n",
    "        \"Remove Rotatable Bonds\"\n",
    "    ]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(labels[::-1], acc_values[::-1], color='skyblue')\n",
    "    plt.xlabel(\"Accuracy After Feature Removal\")\n",
    "    plt.ylabel(\"Feature Removed\")\n",
    "    plt.title(f\"Ablation Study - {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JY1E-qttyZh3",
    "outputId": "6510f0ca-0e7c-4c6c-b04f-8a2b61da015f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Shuffle and clean data\n",
    "df = shuffle(df, random_state=42)\n",
    "y = df[\"count_binned_custom\"].values\n",
    "\n",
    "numeric_columns = ['Exact Mass', 'XLogP3', 'Heavy Atom Count', 'Ring Count',\n",
    "                   'Hydrogen Bond Acceptor Count', 'Hydrogen Bond Donor Count',\n",
    "                   'Rotatable Bond Count', 'Topological Polar Surface Area']\n",
    "\n",
    "additional_columns = []\n",
    "\n",
    "# Combine selected columns with fingerprints\n",
    "df_combined = new_df[numeric_columns + additional_columns].copy().astype(float)\n",
    "X = np.vstack(df[\"Features5\"].values)\n",
    "X_combined = np.hstack((df_combined.values, X))\n",
    "\n",
    "X_all = np.hstack((X_combined, fingerprints))\n",
    "X_all = np.nan_to_num(X_all)\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500, random_state=42),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluation function\n",
    "def compute_cv_accuracy(X, y, clf):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    X_clean = np.nan_to_num(X)\n",
    "    return np.mean(cross_val_score(clf, X_clean, y, cv=skf, scoring='accuracy'))\n",
    "\n",
    "# Feature dimensions\n",
    "fp_len = len(fingerprints[0])\n",
    "node_dim = 94 * 4\n",
    "edge_dim = 42 * 4\n",
    "\n",
    "# Labels for ablation steps\n",
    "labels = [\n",
    "    \"Baseline\",\n",
    "    \"Remove Fingerprint\",\n",
    "    \"Remove Exact Mass\",\n",
    "    \"Remove XLogP3\",\n",
    "    \"Remove Ring Count\",\n",
    "    \"Remove PAGTN Edge Feature\",\n",
    "    \"Remove PAGTN Node Feature\",\n",
    "    'Remove Hydrogen Bond Acceptor Count',\n",
    "    'Remove Hydrogen Bond Donor Count',\n",
    "    \"Remove Rotatable Bonds\"\n",
    "]\n",
    "\n",
    "# Store results: rows = ablation, columns = model names\n",
    "ablation_results = defaultdict(list)\n",
    "\n",
    "# Ablation loop\n",
    "for model_name, clf in models.items():\n",
    "    acc_values = []\n",
    "\n",
    "    # Baseline\n",
    "    acc_values.append(compute_cv_accuracy(X_all, y, clf))\n",
    "\n",
    "    # Remove Fingerprints\n",
    "    X_step = np.delete(X_all, np.s_[-fp_len:], axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_step, y, clf))\n",
    "\n",
    "    # Remove Exact Mass\n",
    "    X_step = np.delete(X_step, 0, axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_step, y, clf))\n",
    "\n",
    "    # Remove XLogP3\n",
    "    X_step = np.delete(X_step, 0, axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_step, y, clf))\n",
    "\n",
    "    # Remove Ring Count (original index 3 in numeric list)\n",
    "    X_step = np.delete(X_step, 1, axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_step, y, clf))\n",
    "\n",
    "    # Remove PAGTN Edge Features\n",
    "    X_step = np.delete(X_step, np.s_[-edge_dim:], axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_step, y, clf))\n",
    "\n",
    "    # Remove PAGTN Node Features\n",
    "    X_step = np.delete(X_step, np.s_[-node_dim:], axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_step, y, clf))\n",
    "\n",
    "    # Remove Hydrogen Bond Acceptor Count\n",
    "    X_step = np.delete(X_step, 1, axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_step, y, clf))\n",
    "\n",
    "    # Remove Hydrogen Bond Donor Count\n",
    "    X_step = np.delete(X_step, 1, axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_step, y, clf))\n",
    "\n",
    "    # Remove Rotatable Bonds\n",
    "    X_step = np.delete(X_step, 1, axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_step, y, clf))\n",
    "\n",
    "    # Store results\n",
    "    for label, acc in zip(labels, acc_values):\n",
    "        ablation_results[label].append(acc)\n",
    "\n",
    "    # Optional: plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(labels[::-1], acc_values[::-1], color='skyblue')\n",
    "    plt.xlabel(\"Accuracy After Feature Removal\")\n",
    "    plt.ylabel(\"Feature Removed\")\n",
    "    plt.title(f\"Ablation Study - {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "# Convert to DataFrame\n",
    "ablation_df = pd.DataFrame(ablation_results, index=models.keys()).T\n",
    "\n",
    "# Save to CSV\n",
    "ablation_df.to_csv(\"ablation_results_table.csv\")\n",
    "\n",
    "# Optional: also save to JSON\n",
    "with open(\"ablation_results_table.json\", \"w\") as f:\n",
    "    json.dump(ablation_results, f, indent=4)\n",
    "\n",
    "print(\"Ablation results saved to 'ablation_results_table.csv' and JSON.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fx1Ifm56mOJ8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "\n",
    "# Collect results by ablation (row = ablation step, col = model name)\n",
    "ablation_results = defaultdict(list)\n",
    "\n",
    "# Shuffle and clean data\n",
    "df = shuffle(df, random_state=42)\n",
    "y = df[\"count_binned_custom\"].values\n",
    "\n",
    "numeric_columns = ['Exact Mass', 'XLogP3', 'Heavy Atom Count', 'Ring Count',\n",
    "                   'Hydrogen Bond Acceptor Count', 'Hydrogen Bond Donor Count',\n",
    "                   'Rotatable Bond Count', 'Topological Polar Surface Area']\n",
    "\n",
    "additional_columns = []\n",
    "\n",
    "# Combine the selected columns with the \"Features\" (fingerprints) to form the input data (X)\n",
    "df_combined = new_df[numeric_columns + additional_columns].copy()\n",
    "df_combined = df_combined.astype(float)\n",
    "\n",
    "# Ensure that 'Features' contains the fingerprint data and combine it\n",
    "X = np.vstack(df[\"Features5\"].values)  # Features from RDKit or other representation\n",
    "X_combined = np.hstack((df_combined.values, X))  # Combine the features\n",
    "\n",
    "X_all = np.hstack((X_combined, fingerprints))\n",
    "X_all = np.nan_to_num(X_all)\n",
    "# Models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500, random_state=42),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Accuracy evaluation\n",
    "def compute_cv_accuracy(X, y, clf):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    X_clean = np.nan_to_num(X)  # Ensure no NaNs\n",
    "    return np.mean(cross_val_score(clf, X, y, cv=skf, scoring='accuracy'))\n",
    "\n",
    "def compute_cv_auc(X, y_bin, clf):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    y_proba = cross_val_predict(clf, X, y, cv=skf, method='predict_proba')\n",
    "    return roc_auc_score(y_bin, y_proba, average=\"macro\", multi_class=\"ovr\")\n",
    "\n",
    "# Feature index helpers\n",
    "fp_len = len(fingerprints[0])\n",
    "node_dim = 94 * 4\n",
    "edge_dim = 42 * 4\n",
    "\n",
    "# Run ablation per model\n",
    "for model_name, clf in models.items():\n",
    "    acc_values = []\n",
    "\n",
    "    # Baseline\n",
    "    acc_values.append(compute_cv_accuracy(X_all, y, clf))\n",
    "\n",
    "    # Remove Fingerprints\n",
    "    X_no_fp = np.delete(X_all, np.s_[-fp_len:], axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_no_fp, y, clf))\n",
    "\n",
    "    # Remove Exact Mass\n",
    "    X_ablated = np.delete(X_no_fp, 0, axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_ablated, y, clf))\n",
    "\n",
    "    # Remove XLogP3\n",
    "    X_ablated = np.delete(X_ablated, 0, axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_ablated, y, clf))\n",
    "\n",
    "    # Remove Ring Count (index 3 in your numeric list)\n",
    "    X_ablated = np.delete(X_ablated, 1, axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_ablated, y, clf))\n",
    "\n",
    "    # Remove PAGTN Edge Features\n",
    "    X_ablated = np.delete(X_ablated, np.s_[- edge_dim:], axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_ablated, y, clf))\n",
    "\n",
    "    # Remove PAGTN Node Features\n",
    "    X_ablated = np.delete(X_ablated, np.s_[-node_dim:], axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_ablated, y, clf))\n",
    "\n",
    "    # Remove Hydrogen Bond Acceptor Count (index 4)\n",
    "    X_ablated = np.delete(X_ablated, 1, axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_ablated, y, clf))\n",
    "\n",
    "    # Remove Hydrogen Bond Donor Count (index 5)\n",
    "    X_ablated = np.delete(X_ablated, 1, axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_ablated, y, clf))\n",
    "\n",
    "    # Remove Rotatable Bonds (index 6)\n",
    "    X_ablated = np.delete(X_ablated, 1, axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_ablated, y, clf))\n",
    "\n",
    "    # Labels in specified order\n",
    "    labels = [\n",
    "        \"Baseline\",\n",
    "        \"Remove Fingerprint\",\n",
    "        \"Remove Exact Mass\",\n",
    "        \"Remove XLogP3\",\n",
    "        \"Remove Ring Count\",\n",
    "        \"Remove PAGTN Edge Feature\",\n",
    "        \"Remove PAGTN Node Feature\",\n",
    "        'Remove Hydrogen Bond Acceptor Count',\n",
    "        'Remove Hydrogen Bond Donor Count',\n",
    "        \"Remove Rotatable Bonds\"\n",
    "    ]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(labels[::-1], acc_values[::-1], color='skyblue')\n",
    "    plt.xlabel(\"Accuracy After Feature Removal\")\n",
    "    plt.ylabel(\"Feature Removed\")\n",
    "    plt.title(f\"Ablation Study - {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrEWrnEevLvf"
   },
   "outputs": [],
   "source": [
    "# Accuracy after removing features\n",
    "acc_values = []\n",
    "X_ablated = X_all.copy()\n",
    "\n",
    "# Remove fingerprints\n",
    "X_ablated_fingerprints = np.delete(X_ablated, np.s_[-len(fingerprints[0]):], axis=1)\n",
    "acc_values.append(compute_cv_accuracy(X_ablated_fingerprints, y, clf))  # 0: Fingerprints\n",
    "\n",
    "# Accuracy after removing each numeric feature one by one\n",
    "for i in range(len(numeric_columns)):\n",
    "    X_temp = np.delete(X_all, i, axis=1)\n",
    "    acc_values.append(compute_cv_accuracy(X_temp, y, clf))  # 1~6: Each numeric feature\n",
    "\n",
    "# Remove node features (assumes they are first 94*4 columns in Features5)\n",
    "X_ablated_node = np.delete(X_ablated_fingerprints, np.s_[:94*4], axis=1)\n",
    "acc_values.insert(3, compute_cv_accuracy(X_ablated_node, y, clf))  # 3: Node Features\n",
    "\n",
    "# Remove edge features (assumes they are last 42*4 columns in Features5)\n",
    "X_ablated_edge = np.delete(X_ablated_node, np.s_[-42*4:], axis=1)\n",
    "acc_values.insert(4, compute_cv_accuracy(X_ablated_edge, y, clf))  # 4: Edge Features\n",
    "\n",
    "# Labels aligned with new acc_values\n",
    "labels = ['Fingerprints'] + numeric_columns[:2] + ['Node Features', 'Edge Features'] + numeric_columns[2:]\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(labels[::-1], acc_values[::-1], color='coral')\n",
    "plt.xlabel(\"Accuracy After Feature Removal\")\n",
    "plt.ylabel(\"Features Removed\")\n",
    "plt.title(\"Ablation Study: Accuracy After Removing Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FbqNL20r-Fz"
   },
   "outputs": [],
   "source": [
    "len(X_ablated_edge[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkW-XrC9pIt0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.utils import shuffle\n",
    "from rdkit.Chem import AllChem as Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "\n",
    "# Shuffle and prepare data\n",
    "df = shuffle(df, random_state=42)\n",
    "df['count_binned_custom'] = pd.cut(df['number_dedup'], bins=[0, 20.1, 60.1, 115], labels=[0, 1, 2]).astype(int)\n",
    "\n",
    "# Compute Morgan fingerprints\n",
    "mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=128)\n",
    "def compute_morgan_fingerprint(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    return np.array(mfpgen.GetFingerprint(mol)) if mol else None\n",
    "\n",
    "fingerprints = [compute_morgan_fingerprint(smile) for smile in df[\"SMILES\"]]\n",
    "fingerprints = np.array([f for f in fingerprints if f is not None])\n",
    "\n",
    "# Filter df to match fingerprint count\n",
    "df = df.iloc[:len(fingerprints)].reset_index(drop=True)\n",
    "\n",
    "# Numeric features\n",
    "numeric_columns = ['Exact Mass', 'XLogP3', 'Heavy Atom Count', 'Ring Count',\n",
    "                   'Hydrogen Bond Acceptor Count', 'Hydrogen Bond Donor Count',\n",
    "                   'Rotatable Bond Count', 'Topological Polar Surface Area']\n",
    "\n",
    "# Combine the selected columns with the \"Features\" (fingerprints) to form the input data (X)\n",
    "df_combined = new_df[numeric_columns].copy()\n",
    "df_combined = df_combined.astype(float)\n",
    "\n",
    "# Ensure that 'Features' contains the fingerprint data and combine it\n",
    "df_combined = np.array(df_combined)\n",
    "y = df[\"count_binned_custom\"].values\n",
    "y_bin = label_binarize(y, classes=[0, 1, 2])\n",
    "\n",
    "# Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=200, min_samples_split=10,\n",
    "                             class_weight=\"balanced\", random_state=42)\n",
    "# Compute baseline accuracy\n",
    "def compute_cv_accuracy(X, y, clf):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    y_pred = cross_val_predict(clf, X, y, cv=skf)\n",
    "    return accuracy_score(y, y_pred)\n",
    "\n",
    "baseline_accuracy = compute_cv_accuracy(X_all, y, clf)\n",
    "\n",
    "# Accuracy after removing each numeric feature (one by one)\n",
    "X_ablated = df_combined\n",
    "\n",
    "# # Accuracy after removing node features (assuming node features come before fingerprints)\n",
    "# X_ablated_node = np.delete(X_ablated, np.s_[94*4:136*4], axis=1)  # Adjust if necessary\n",
    "# accuracy_values.append(compute_cv_accuracy(X_ablated_node, y, clf))\n",
    "\n",
    "# # Accuracy after removing node features (assuming node features come before fingerprints)\n",
    "# X_ablated_edge = np.delete(X_ablated_node, np.s_[:94*4], axis=1)  # Adjust if necessary\n",
    "# accuracy_values.append(compute_cv_accuracy(X_ablated_edge, y, clf))\n",
    "\n",
    "# # Accuracy after removing fingerprints\n",
    "# X_ablated_fingerprints = np.delete(X_ablated_edge, np.s_[-len(fingerprints[0]):], axis=1)\n",
    "# accuracy_values.append(compute_cv_accuracy(X_ablated_fingerprints, y, clf))\n",
    "\n",
    "accuracy_values = []\n",
    "for i, col in enumerate(numeric_columns):\n",
    "    if i == 7:\n",
    "      break\n",
    "    X_ablated = np.delete(X_ablated, 7-i, axis=1)\n",
    "    accuracy_values.append(compute_cv_accuracy(X_ablated, y, clf))\n",
    "\n",
    "# Plotting\n",
    "labels = ['Exact Mass', 'XLogP3', 'Heavy Atom Count', 'Ring Count',\n",
    "                   'Hydrogen Bond Acceptor Count', 'Hydrogen Bond Donor Count',\n",
    "                   'Rotatable Bond Count']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(labels[::-1], accuracy_values[::-1], color='mediumseagreen')\n",
    "plt.xlabel(\"Accuracy After Feature Removal\")\n",
    "plt.ylabel(\"Removed Feature\")\n",
    "plt.title(\"Ablation Study: Accuracy After Removing Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbheLTwZYiXR"
   },
   "outputs": [],
   "source": [
    "len(X_ablated_fingerprints[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPjVlm9DYyfD"
   },
   "outputs": [],
   "source": [
    "len(X_ablated_node[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csObG-AaZ0dC"
   },
   "outputs": [],
   "source": [
    "len(df_combined[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjIzYM_SXfSf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.utils import shuffle\n",
    "from rdkit.Chem import AllChem as Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "\n",
    "# Shuffle and prepare data\n",
    "df = shuffle(df, random_state=42)\n",
    "df['count_binned_custom'] = pd.cut(df['number_dedup'], bins=[0, 20.1, 60.1, 115], labels=[0, 1, 2]).astype(int)\n",
    "\n",
    "# Compute Morgan fingerprints\n",
    "mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=128)\n",
    "def compute_morgan_fingerprint(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    return np.array(mfpgen.GetFingerprint(mol)) if mol else None\n",
    "\n",
    "fingerprints = [compute_morgan_fingerprint(smile) for smile in df[\"SMILES\"]]\n",
    "fingerprints = [f for f in fingerprints if f is not None]\n",
    "fingerprints = np.array(fingerprints)\n",
    "\n",
    "# Filter df to match fingerprint count\n",
    "df = df.iloc[:len(fingerprints)].reset_index(drop=True)\n",
    "\n",
    "# Numeric features\n",
    "numeric_columns = ['Exact Mass', 'XLogP3', 'Heavy Atom Count', 'Ring Count',\n",
    "                   'Hydrogen Bond Acceptor Count', 'Hydrogen Bond Donor Count',\n",
    "                   'Rotatable Bond Count', 'Topological Polar Surface Area']\n",
    "\n",
    "# Combine the selected columns with the \"Features\" (fingerprints) to form the input data (X)\n",
    "df_combined = new_df[numeric_columns].copy()\n",
    "df_combined = df_combined.fillna(0)\n",
    "df_combined = df_combined.astype(float)\n",
    "\n",
    "# Ensure that 'Features' contains the fingerprint data and combine it\n",
    "df[\"Features5\"] = df[\"Features5\"].fillna(0)\n",
    "X = np.vstack(df[\"Features5\"].values)  # Features from RDKit or other representation\n",
    "X_combined = np.hstack((df_combined.values, X))  # Combine the numeric and aggregated features\n",
    "df_combined = df_combined.fillna(0)\n",
    "X_all = np.hstack((X_combined, fingerprints))  # Combine with fingerprints\n",
    "X_all = np.nan_to_num(X_all)  # Replace NaN with 0\n",
    "y = df[\"count_binned_custom\"].values\n",
    "y_bin = label_binarize(y, classes=[0, 1, 2])\n",
    "\n",
    "# Define all models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=200, min_samples_split=10,\n",
    "                                            class_weight=\"balanced\", random_state=42),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500, random_state=42)\n",
    "}\n",
    "\n",
    "# Compute baseline AUC for each model\n",
    "def compute_cv_auc(X, y_bin, clf):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    y_proba = cross_val_predict(clf, X, y, cv=skf, method='predict_proba')\n",
    "    return roc_auc_score(y_bin, y_proba, average=\"macro\", multi_class=\"ovr\")\n",
    "\n",
    "# Plotting for each model\n",
    "for model_name, clf in models.items():\n",
    "    baseline_auc = compute_cv_auc(X_all, y_bin, clf)\n",
    "\n",
    "    # AUC after removing each numeric feature (one by one)\n",
    "    X_ablated = X_all\n",
    "    auc_values = []\n",
    "    for i, col in enumerate(numeric_columns):\n",
    "        mask = [j for j in range(X_ablated.shape[1]) if j != i]  # skip this numeric feature\n",
    "        X_ablated = np.delete(X_ablated, i, axis=1)\n",
    "        auc_values.append(compute_cv_auc(X_ablated, y_bin, clf))  # AUC after removing one numeric feature\n",
    "\n",
    "    # AUC after removing fingerprints (all at once)\n",
    "    X_ablated_fingerprints = np.delete(X_ablated, np.s_[-len(fingerprints[0]):], axis=1)  # Remove fingerprint columns\n",
    "    auc_values.append(compute_cv_auc(X_ablated_fingerprints, y_bin, clf))  # AUC without fingerprints\n",
    "\n",
    "    # AUC after removing node features (all at once, assuming node features are in \"Features5\" columns 0-4)\n",
    "    X_ablated_node = np.delete(X_ablated_fingerprints, np.s_[:4], axis=1)  # Remove node features\n",
    "    auc_values.append(compute_cv_auc(X_ablated_node, y_bin, clf))  # AUC without node features\n",
    "\n",
    "    # Plotting\n",
    "    labels = numeric_columns + ['Fingerprints', 'Node Features']\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(labels[::-1], auc_values[::-1], color='teal')\n",
    "    plt.xlabel(\"AUC After Feature Removal\")\n",
    "    plt.ylabel(\"Features\")\n",
    "    plt.title(f\"Ablation Study: AUC After Removing Features - {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOOuZWKvOvvV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Updated evaluate function with single ROC curve for macro-average\n",
    "def evaluate_auc(model, loader, num_classes):\n",
    "    model.eval()\n",
    "    all_probs, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            out = model(batch)  # shape: [batch_size, num_classes]\n",
    "            probs = F.softmax(out, dim=1)\n",
    "            all_probs.append(probs.cpu())\n",
    "            all_labels.append(batch.y.cpu())\n",
    "\n",
    "    probs = torch.cat(all_probs).numpy()\n",
    "    labels = torch.cat(all_labels).numpy()\n",
    "    labels_bin = label_binarize(labels, classes=list(range(num_classes)))\n",
    "\n",
    "    # Compute macro-average AUC\n",
    "    auc = roc_auc_score(labels_bin, probs, average=\"macro\", multi_class=\"ovr\")\n",
    "\n",
    "    # Compute macro-average ROC curve\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels_bin[:, i], probs[:, i])\n",
    "\n",
    "    # Compute all fpr points\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at these points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(num_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    mean_tpr /= num_classes\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(all_fpr, mean_tpr, label=f'Macro-average ROC curve (AUC = {auc:.4f})', color='b')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Macro-Average ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gFXY_n9OyoV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "X_all = np.nan_to_num(X_all)  # Replace NaN with 0\n",
    "\n",
    "# Assume X_all (features), y (labels), y_bin (one-hot labels) are defined\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500, random_state=42)\n",
    "}\n",
    "\n",
    "# Plot settings\n",
    "plt.figure(figsize=(10, 8))\n",
    "df['count_binned_custom'] = pd.cut(df['number_dedup'], bins=[0, 16, 60, 115], labels=[0, 1, 2]).astype(int)\n",
    "y = df['count_binned_custom']\n",
    "# Cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Plot ROC curve for each model\n",
    "for name, model in models.items():\n",
    "    # Get predicted probabilities\n",
    "    y_proba = cross_val_predict(model, X_all, y, cv=skf, method='predict_proba')\n",
    "\n",
    "    # Compute micro-average ROC\n",
    "    fpr, tpr, _ = roc_curve(y_bin.ravel(), y_proba.ravel())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "# Diagonal line\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "\n",
    "# Labels and legend\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Macro-Averaged ROC Curve (3-Class)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMUDcQXZlNRI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Generate synthetic data (replace with your X, y)\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=3, n_informative=6, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Binarize labels for OvR\n",
    "classes = [0, 1, 2]\n",
    "y_test_bin = label_binarize(y_test, classes=classes)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500, random_state=42)\n",
    "}\n",
    "\n",
    "# Plot macro-average ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "for name, model in models.items():\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Get probabilities\n",
    "    y_score = model.predict_proba(X_test)\n",
    "\n",
    "    # Compute macro-average ROC\n",
    "    fpr_grid = np.linspace(0, 1, 100)\n",
    "    mean_tpr = np.zeros_like(fpr_grid)\n",
    "    aucs = []\n",
    "    for i in range(len(classes)):\n",
    "        fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "        mean_tpr += np.interp(fpr_grid, fpr, tpr)\n",
    "        aucs.append(roc_auc_score(y_test_bin[:, i], y_score[:, i]))\n",
    "    mean_tpr /= len(classes)\n",
    "    macro_auc = np.mean(aucs)\n",
    "\n",
    "    # Plot ROC\n",
    "    plt.plot(fpr_grid, mean_tpr, label=f\"{name} (Macro AUC = {macro_auc:.2f})\")\n",
    "\n",
    "# Plot diagonal line\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Macro-Average ROC Curves for Multi-Class Classification')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show plot\n",
    "plt.savefig('roc_curves_macro.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXiwjhy3n2ja"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import RocCurveDisplay, roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import seaborn as sns\n",
    "\n",
    "# Define pmeague function (macro-average AUC)\n",
    "def pmeague(y_true, y_score):\n",
    "    n_classes = y_score.shape[1]\n",
    "    aucs = []\n",
    "    for i in range(n_classes):\n",
    "        auc = roc_auc_score(y_true[:, i], y_score[:, i])\n",
    "        aucs.append(auc)\n",
    "    return np.mean(aucs)\n",
    "\n",
    "# Generate synthetic data (replace with your X, y)\n",
    "X = np.vstack(df[\"Features5\"].values)  # Features from RDKit or other representation\n",
    "X = np.nan_to_num(X)\n",
    "X = X_all\n",
    "df['count_binned_custom'] = pd.cut(df['number_dedup'], bins=[0, 20, 60, 115], labels=[0, 1, 2]).astype(int)\n",
    "y = df['count_binned_custom']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Binarize labels for OvR\n",
    "classes = [0, 1, 2]\n",
    "y_test_bin = label_binarize(y_test, classes=classes)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500, random_state=42)\n",
    "}\n",
    "\n",
    "# Color palette\n",
    "palette = sns.color_palette(\"Set2\", len(models))\n",
    "\n",
    "# Plot macro-average ROC curves\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "for i, (model_name, model) in enumerate(models.items()):\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Get predictions\n",
    "    model_predictions = model.predict_proba(X_test)\n",
    "\n",
    "    # Compute macro-average AUC\n",
    "    auc = pmeague(y_test_bin, model_predictions)\n",
    "\n",
    "    # Compute macro-average ROC\n",
    "    fpr_grid = np.linspace(0, 1, 100)\n",
    "    mean_tpr = np.zeros_like(fpr_grid)\n",
    "    for j in range(len(classes)):\n",
    "        fpr, tpr, _ = roc_curve(y_test_bin[:, j], model_predictions[:, j])\n",
    "        mean_tpr += np.interp(fpr_grid, fpr, tpr)\n",
    "    mean_tpr /= len(classes)\n",
    "\n",
    "    # Plot ROC with RocCurveDisplay\n",
    "    display = RocCurveDisplay(\n",
    "        fpr=fpr_grid,\n",
    "        tpr=mean_tpr,\n",
    "        roc_auc=auc,\n",
    "        estimator_name=model_name\n",
    "    )\n",
    "    display.plot(\n",
    "        ax=ax,\n",
    "        label=f\"{model_name} (AUC = {auc:.3f})\",\n",
    "        color=palette[i],\n",
    "        plot_chance_level=True if i == len(models) - 1 else False,\n",
    "        linewidth=1.5\n",
    "    )\n",
    "\n",
    "# Customize plot\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Macro-Average ROC Curves for Multi-Class Classification')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True)\n",
    "\n",
    "# Save and show plot\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curves_macro.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IT9wXxlHfsgH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Replace NaN\n",
    "X_all = np.nan_to_num(X_all)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500, random_state=42)\n",
    "}\n",
    "\n",
    "# Plot setup\n",
    "plt.figure(figsize=(10, 8))\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Plot ROC for each baseline model\n",
    "for name, model in models.items():\n",
    "    y_proba = cross_val_predict(model, X_all, y, cv=skf, method='predict_proba')\n",
    "    fpr, tpr, _ = roc_curve(y_bin.ravel(), y_proba.ravel())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "# Load and plot GAT ROC curve\n",
    "with open(\"gat_roc.pkl\", 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "gat_fpr, gat_tpr, gat_auc, gat_label = data['fpr'], data['tpr'], data['auc'], data['label']\n",
    "plt.plot(gat_fpr, gat_tpr, label=f\"{gat_label} (AUC = {gat_auc:.3f})\", linestyle='--', color='purple')\n",
    "\n",
    "# Random line\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "\n",
    "# Final plot settings\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Micro-Average ROC Curve Comparison\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiMmsgoqXt69"
   },
   "source": [
    "###AUC is done here, I try fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Duxxgw74jsDj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "\n",
    "# Load your dataframe here\n",
    "# df = pd.read_csv(\"your_data.csv\")  # Ensure it has 'SMILES', 'Features5', and numeric columns\n",
    "\n",
    "importance_data = []\n",
    "\n",
    "dimensions = [8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "for dim in dimensions:\n",
    "    mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=dim)\n",
    "\n",
    "    def compute_morgan_fingerprint(smiles):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        return np.array(mfpgen.GetFingerprint(mol)) if mol else np.zeros(dim)\n",
    "\n",
    "    fingerprints = np.array([compute_morgan_fingerprint(smiles) for smiles in df[\"SMILES\"]])\n",
    "    fingerprints = np.nan_to_num(fingerprints)\n",
    "\n",
    "    X_numeric = new_df[numeric_columns].fillna(0).values\n",
    "    X_node_edge = np.vstack(df[\"Features5\"].values)  # Assumes shape (N, 100) or similar\n",
    "    X_all = np.hstack((X_numeric, X_node_edge, fingerprints))\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_all, y)\n",
    "    importances = clf.feature_importances_\n",
    "\n",
    "    fingerprint_importance = importances[-dim:]\n",
    "    total_importance = np.sum(fingerprint_importance)\n",
    "    mean_importance = np.mean(fingerprint_importance)\n",
    "\n",
    "    importance_data.append([dim, total_importance, mean_importance])\n",
    "\n",
    "# Save to CSV\n",
    "importance_df = pd.DataFrame(importance_data, columns=[\"Dimension\", \"Sum Importance\", \"Mean Importance\"])\n",
    "importance_df.to_csv(\"fingerprint_importance_summary.csv\", index=False)\n",
    "print(\"Saved to fingerprint_importance_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIkg43e8kHLU"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame and \"Features\" contains the fingerprint features\n",
    "# Combine numeric features from df with existing fingerprint features (X)\n",
    "\n",
    "# Extract numeric columns from the DataFrame\n",
    "numeric_columns = ['Exact Mass', 'XLogP3', 'Heavy Atom Count', 'Ring Count',\n",
    "                   'Hydrogen Bond Acceptor Count', 'Hydrogen Bond Donor Count',\n",
    "                   'Rotatable Bond Count', 'Topological Polar Surface Area']\n",
    "\n",
    "# Additional columns to include (if any)\n",
    "additional_columns = []\n",
    "\n",
    "# Combine the selected columns with the \"Features\" (fingerprints) to form the input data (X)\n",
    "df_combined = new_df[numeric_columns + additional_columns].copy()\n",
    "df_combined = df_combined.astype(float)\n",
    "\n",
    "# Ensure that 'Features' contains the fingerprint data and combine it\n",
    "X = np.vstack(df[\"Features5\"].values)  # Features from RDKit or other representation\n",
    "X_combined = np.hstack((df_combined.values, X))  # Combine the features\n",
    "\n",
    "# Target variable\n",
    "y = df['count_binned_custom'].values  # Target labels\n",
    "\n",
    "# Train-test split (optional but common to have)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=200, min_samples_split=10,\n",
    "                             class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importance visualization\n",
    "# Combine numeric and additional columns for feature names\n",
    "feature_names = numeric_columns + additional_columns + ['Feature_' + str(i) for i in range(X.shape[1])]\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Filter out only the newly added features\n",
    "new_feature_importance_df = feature_importance_df[feature_importance_df['Feature'].isin(numeric_columns + additional_columns)]\n",
    "\n",
    "# Print the sorted feature importances  (for newly added features)\n",
    "print(new_feature_importance_df)\n",
    "\n",
    "# Optionally, visualize the feature importances for the newly added features\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(new_feature_importance_df['Feature'], new_feature_importance_df['Importance'])\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Importance for Newly Added Features from RandomForest Classifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I76Iek5hl7Sd"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Numeric columns from the DataFrame\n",
    "numeric_columns = ['Exact Mass', 'XLogP3', 'Heavy Atom Count', 'Ring Count',\n",
    "                   'Hydrogen Bond Acceptor Count', 'Hydrogen Bond Donor Count',\n",
    "                   'Rotatable Bond Count', 'Topological Polar Surface Area']\n",
    "\n",
    "# Node + edge features (Features5), assumed to be a numpy array per row\n",
    "node_edge_features = np.vstack(df[\"Features5\"].values)  # shape (N, D_node_edge)\n",
    "\n",
    "# Combine numeric columns\n",
    "df_combined = df[numeric_columns].copy()\n",
    "df_combined = df_combined.astype(float)\n",
    "X_numeric = df_combined.values\n",
    "\n",
    "# Fingerprints (already computed and stored in \"fingerprints\")\n",
    "X_fingerprints = np.array([f for f in df[\"Features\"].values])  # You may also use \"fingerprints\" from earlier steps\n",
    "\n",
    "# Combine all features\n",
    "X_all = np.hstack((X_numeric, node_edge_features, X_fingerprints))  # Final input\n",
    "y = df['count_binned_custom'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train RF model\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=200, min_samples_split=10,\n",
    "                             class_weight=\"balanced\", random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "# Calculate fingerprint importance stats\n",
    "num_numeric = X_numeric.shape[1]\n",
    "num_node_edge = node_edge_features.shape[1]\n",
    "num_fp = X_fingerprints.shape[1]\n",
    "\n",
    "fp_importances = importances[-num_fp:]\n",
    "fp_importance_sum = fp_importances.sum()\n",
    "fp_importance_mean = fp_importances.mean()\n",
    "\n",
    "print(f\"Fingerprint Importance (sum): {fp_importance_sum:.4f}\")\n",
    "print(f\"Fingerprint Importance (mean): {fp_importance_mean:.6f}\")\n",
    "\n",
    "# Save summary to CSV\n",
    "fp_dim = num_fp\n",
    "summary_df = pd.DataFrame([{\n",
    "    \"Fingerprint Dimension\": fp_dim,\n",
    "    \"Sum Importance\": fp_importance_sum,\n",
    "    \"Mean Importance\": fp_importance_mean\n",
    "}])\n",
    "summary_df.to_csv(\"fingerprint_feature_importance_summary.csv\", index=False)\n",
    "print(\"Saved fingerprint importance summary to CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOXi5-jUfhY1"
   },
   "outputs": [],
   "source": [
    "new_feature_importance_df['Importance'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-37pi2BnINY"
   },
   "outputs": [],
   "source": [
    "# Check the data types of columns\n",
    "print(new_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6uDNC9QpIpA"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming df is your DataFrame and \"Features\" contains the fingerprint features\n",
    "# Combine numeric features from df with existing fingerprint features (X)\n",
    "\n",
    "# Extract numeric columns from the DataFrame\n",
    "numeric_columns = ['Exact Mass', 'XLogP3', 'Heavy Atom Count', 'Ring Count',\n",
    "                   'Hydrogen Bond Acceptor Count', 'Hydrogen Bond Donor Count',\n",
    "                   'Rotatable Bond Count', 'Topological Polar Surface Area']\n",
    "\n",
    "# Additional columns to include (if any)\n",
    "additional_columns = []\n",
    "\n",
    "# Combine the selected columns with the \"Features\" (fingerprints) to form the input data (X)\n",
    "df_combined = new_df[numeric_columns + additional_columns].copy()\n",
    "df_combined = df_combined.astype(float)\n",
    "\n",
    "# Ensure that 'Features' contains the fingerprint data and combine it\n",
    "X = np.vstack(df[\"Features5\"].values)  # Features from RDKit or other representation\n",
    "X_combined = np.hstack((df_combined.values, X))  # Combine the features\n",
    "\n",
    "# Target variable\n",
    "y = df['count_binned_custom'].values  # Target labels\n",
    "\n",
    "# Train-test split (optional but common to have)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=200, min_samples_split=10,\n",
    "                             class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importance visualization\n",
    "feature_names = numeric_columns + additional_columns + [str(i) for i in range(X.shape[1])]\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Filter out features with importance <= 0\n",
    "feature_importance_df = feature_importance_df[feature_importance_df['Importance'] > 0]\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print feature importances for all features greater than 0\n",
    "print(\"Feature Importances (greater than 0):\")\n",
    "for feature, importance in zip(feature_importance_df['Feature'], feature_importance_df['Importance']):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# Visualizing the feature importances vertically\n",
    "plt.figure(figsize=(24, 6))\n",
    "\n",
    "# Color the bars (orange for numeric columns, blue for others)\n",
    "colors = ['orange' if feature in numeric_columns else 'blue' for feature in feature_importance_df['Feature']]\n",
    "\n",
    "# Plot the bars\n",
    "bars = plt.bar(feature_importance_df['Feature'], feature_importance_df['Importance'], color=colors)\n",
    "\n",
    "# Set x-labels to blank for non-numeric features (blue)\n",
    "for i, feature in enumerate(feature_importance_df['Feature']):\n",
    "    if feature not in numeric_columns:\n",
    "        bars[i].set_label(' ')  # Set blank label for blue bars\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Labels and title\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.title(\"Feature Importance with Highlighted Numeric Features (Importance > 0)\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQya9xXVt0Sp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have the data in 'new_df' DataFrame\n",
    "numeric_columns = ['Exact Mass', 'XLogP3', 'Heavy Atom Count', 'Ring Count',\n",
    "                   'Hydrogen Bond Acceptor Count', 'Hydrogen Bond Donor Count',\n",
    "                   'Rotatable Bond Count', 'Topological Polar Surface Area']\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = new_df[numeric_columns].corr()\n",
    "\n",
    "# Visualize the correlation matrix using a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix of Numeric Features\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: If you want to test for the significance of these correlations\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Example: Testing correlation between 'Exact Mass' and 'XLogP3'\n",
    "corr, p_value = pearsonr(new_df['Exact Mass'], new_df['XLogP3'])\n",
    "\n",
    "print(f\"Correlation coefficient: {corr}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOnPpCk1gCqx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have the data in 'new_df' DataFrame\n",
    "numeric_columns = ['XLogP3', 'Heavy Atom Count', 'Ring Count',\n",
    "                   'Hydrogen Bond Donor Count',\n",
    "                   'Rotatable Bond Count', 'Topological Polar Surface Area']\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = new_df[numeric_columns].corr()\n",
    "\n",
    "# Visualize the correlation matrix using a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix of Numeric Features\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: If you want to test for the significance of these correlations\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Example: Testing correlation between 'Exact Mass' and 'XLogP3'\n",
    "corr, p_value = pearsonr(new_df['Exact Mass'], new_df['XLogP3'])\n",
    "\n",
    "print(f\"Correlation coefficient: {corr}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SsG1uM9-gn35"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame and \"Features\" contains the fingerprint features\n",
    "# Combine numeric features from df with existing fingerprint features (X)\n",
    "\n",
    "# Extract numeric columns from the DataFrame\n",
    "numeric_columns = ['XLogP3', 'Heavy Atom Count', 'Ring Count',\n",
    "                   'Hydrogen Bond Donor Count',\n",
    "                   'Rotatable Bond Count', 'Topological Polar Surface Area']\n",
    "\n",
    "# Additional columns to include (if any)\n",
    "additional_columns = []\n",
    "\n",
    "# Combine the selected columns with the \"Features\" (fingerprints) to form the input data (X)\n",
    "df_combined = new_df[numeric_columns + additional_columns].copy()\n",
    "df_combined = df_combined.astype(float)\n",
    "\n",
    "# Ensure that 'Features' contains the fingerprint data and combine it\n",
    "X = np.vstack(df[\"Features5\"].values)  # Features from RDKit or other representation\n",
    "X_combined = np.hstack((df_combined.values, X))  # Combine the features\n",
    "\n",
    "# Target variable\n",
    "y = df['count_binned_custom'].values  # Target labels\n",
    "\n",
    "# Train-test split (optional but common to have)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=200, min_samples_split=10,\n",
    "                             class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importance visualization\n",
    "# Combine numeric and additional columns for feature names\n",
    "feature_names = numeric_columns + additional_columns + ['Feature_' + str(i) for i in range(X.shape[1])]\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Filter out only the newly added features\n",
    "new_feature_importance_df = feature_importance_df[feature_importance_df['Feature'].isin(numeric_columns + additional_columns)]\n",
    "\n",
    "# Print the sorted feature importances  (for newly added features)\n",
    "print(new_feature_importance_df)\n",
    "\n",
    "# Optionally, visualize the feature importances for the newly added features\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(new_feature_importance_df['Feature'], new_feature_importance_df['Importance'])\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Importance for Newly Added Features from RandomForest Classifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mug6Uew8hHp8"
   },
   "outputs": [],
   "source": [
    "new_feature_importance_df['Importance'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S90Q5TC7f171"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame and \"Features\" contains the fingerprint features\n",
    "# Combine numeric features from df with existing fingerprint features (X)\n",
    "\n",
    "# Extract numeric columns from the DataFrame\n",
    "numeric_columns = ['XLogP3', 'Heavy Atom Count', 'Ring Count',\n",
    "                   'Hydrogen Bond Acceptor Count', 'Hydrogen Bond Donor Count',\n",
    "                   'Rotatable Bond Count', 'Topological Polar Surface Area']\n",
    "\n",
    "# Additional columns to include (if any)\n",
    "additional_columns = []\n",
    "\n",
    "# Combine the selected columns with the \"Features\" (fingerprints) to form the input data (X)\n",
    "df_combined = new_df[numeric_columns + additional_columns].copy()\n",
    "df_combined = df_combined.astype(float)\n",
    "\n",
    "# Ensure that 'Features' contains the fingerprint data and combine it\n",
    "X = np.vstack(df[\"Features5\"].values)  # Features from RDKit or other representation\n",
    "X_combined = np.hstack((df_combined.values, X))  # Combine the features\n",
    "\n",
    "# Target variable\n",
    "y = df['count_binned_custom'].values  # Target labels\n",
    "\n",
    "# Train-test split (optional but common to have)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=200, min_samples_split=10,\n",
    "                             class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importance visualization\n",
    "# Combine numeric and additional columns for feature names\n",
    "feature_names = numeric_columns + additional_columns + ['Feature_' + str(i) for i in range(X.shape[1])]\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Filter out only the newly added features\n",
    "new_feature_importance_df = feature_importance_df[feature_importance_df['Feature'].isin(numeric_columns + additional_columns)]\n",
    "\n",
    "# Print the sorted feature importances  (for newly added features)\n",
    "print(new_feature_importance_df)\n",
    "\n",
    "# Optionally, visualize the feature importances for the newly added features\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(new_feature_importance_df['Feature'], new_feature_importance_df['Importance'])\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Importance for Newly Added Features from RandomForest Classifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ncmDLrIbsAuZ"
   },
   "outputs": [],
   "source": [
    "numeric_columns = ['XLogP3', 'Ring Count',\n",
    "                   'Hydrogen Bond Donor Count',\n",
    "                   'Rotatable Bond Count']\n",
    "\n",
    "# Additional columns to include (if any)\n",
    "additional_columns = []\n",
    "\n",
    "# Combine the selected columns with the \"Features\" (fingerprints) to form the input data (X)\n",
    "df_combined = new_df[numeric_columns + additional_columns].copy()\n",
    "df_combined = df_combined.astype(float)\n",
    "\n",
    "# Ensure that 'Features' contains the fingerprint data and combine it\n",
    "X = np.vstack(df[\"Features1\"].values)  # Features from RDKit or other representation\n",
    "X_combined = np.hstack((df_combined.values, X))  # Combine the features\n",
    "\n",
    "# Target variable\n",
    "y = df['count_binned_custom'].values  # Target labels\n",
    "\n",
    "# Train-test split (optional but common to have)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=200, min_samples_split=10,\n",
    "                             class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=10, scoring=\"accuracy\")\n",
    "\n",
    "# Print results\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean accuracy:\", scores.mean())\n",
    "print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKloLAyXNUf7"
   },
   "source": [
    "##Feature recommended by chatgpt on [rdkit](https://www.rdkit.org/docs/source/rdkit.Chem.rdMolDescriptors.html)\n",
    "\n",
    "\n",
    "1. BCUT2D Descriptors:\n",
    "\n",
    "Function: BCUT2D(mol)\n",
    "Description: Computes 2D BCUT descriptors, which are eigenvalues derived from a modified adjacency matrix of the molecule. These descriptors incorporate atomic properties such as mass, Gasteiger charge, Crippen logP, and Crippen molar refractivity.\n",
    "Reference: J. Chem. Inf. Comput. Sci., Vol. 39, No. 1, 1999.m\n",
    "2. Molecular Weight (we have)\n",
    "3. Topological Polar Surface Area (TPSA) (we have)\n",
    "4. LogP and Molar Refractivity -- we have XLogP3\n",
    "Function: CalcCrippenDescriptors(mol)\n",
    "Description: Returns the octanol-water partition coefficient (logP) and molar refractivity, indicative of the molecule's hydrophobicity and polarizability, respectively.\n",
    "5. Fractional CSP3:\n",
    "Function: CalcFractionCSP3(mol)\n",
    "Description: Calculates the fraction of carbon atoms that are sp3 hybridized, providing insight into the molecule's saturation level.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_0GSsrNNT5T"
   },
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem.Lipinski import FractionCSP3\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit.Chem.Crippen import MolLogP\n",
    "\n",
    "# Assuming 'df' is your DataFrame and \"Features\" contains the fingerprint features\n",
    "# Define the numeric columns and additional columns\n",
    "numeric_columns = ['Exact Mass', 'XLogP3', 'Topological Polar Surface Area']\n",
    "\n",
    "# Add any additional columns if necessary (can be left empty)\n",
    "additional_columns = []\n",
    "\n",
    "# Function to get molecule from SMILES\n",
    "def get_mol(smiles, radius=2, fpSize=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    return mol\n",
    "\n",
    "# Assuming you have a 'smiles' column in your DataFrame\n",
    "df[\"Mol\"] = [get_mol(smile) for smile in df[\"SMILES\"]]\n",
    "\n",
    "# Function to calculate BCUT2D descriptors\n",
    "def calculate_bcut2d(mol):\n",
    "    # BCUT2D descriptor calculation using atomic properties (mass and LogP)\n",
    "    atom_props = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        mass = atom.GetMass()\n",
    "        charge = MolLogP(mol)  # Use MolLogP from Crippen\n",
    "        atom_props.append(mass + charge)  # Example: sum of mass and LogP\n",
    "    return rdMolDescriptors.BCUT2D(mol, atom_props)\n",
    "\n",
    "# Function to calculate Fractional CSP3\n",
    "def calculate_fsp3(mol):\n",
    "    return FractionCSP3(mol)\n",
    "\n",
    "# Apply the descriptor calculation to each molecule in the DataFrame\n",
    "df[\"BCUT2D\"] = [calculate_bcut2d(mol) for mol in df[\"Mol\"]]\n",
    "df[\"Fractional_CSP3\"] = [calculate_fsp3(mol) for mol in df[\"Mol\"]]\n",
    "\n",
    "# Adding these descriptors to the dataframe\n",
    "df_combined = pd.DataFrame(columns=numeric_columns + additional_columns)\n",
    "\n",
    "# Calculate high and low eigenvalues from BCUT2D descriptor\n",
    "bcut2d_high = [bcut[0] for bcut in df[\"BCUT2D\"]]\n",
    "bcut2d_low = [bcut[1] for bcut in df[\"BCUT2D\"]]\n",
    "\n",
    "df_combined['Exact Mass'] = new_df[\"Exact Mass\"]\n",
    "df_combined['XLogP3'] = new_df[\"XLogP3\"]\n",
    "df_combined['Topological Polar Surface Area'] = new_df[\"Topological Polar Surface Area\"]\n",
    "df_combined['BCUT2D_high'] = bcut2d_high  # High eigenvalue from BCUT2D\n",
    "df_combined['BCUT2D_low'] = bcut2d_low   # Low eigenvalue from BCUT2D\n",
    "df_combined['Fractional_CSP3'] = df[\"Fractional_CSP3\"]    # Fraction of sp³ hybridized carbons\n",
    "\n",
    "# Convert the dataframe to float (if needed)\n",
    "df_combined = df_combined.astype(float)\n",
    "\n",
    "# Ensure that 'Features' contains the fingerprint data and combine it\n",
    "X = np.vstack(df[\"Features\"].values)  # Features from RDKit or other representation\n",
    "X_combined = np.hstack((df_combined.values, X))  # Combine the new descriptors with other features\n",
    "\n",
    "# Target variable (this assumes you have a target variable, e.g., 'count_binned_custom')\n",
    "y = df['count_binned_custom'].values  # Target labels\n",
    "\n",
    "# Train-test split (optional but common to have)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=200, min_samples_split=10,\n",
    "                             class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importance visualization\n",
    "feature_names = numeric_columns + additional_columns + ['Feature_' + str(i) for i in range(X.shape[1])] + ['BCUT2D_high', 'BCUT2D_low', 'Fractional_CSP3']\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance_df = feature_importance_df[feature_importance_df['Importance'] > 0]\n",
    "\n",
    "# Print the sorted feature importances (for newly added features)\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Visualizing the feature importances vertically\n",
    "plt.figure(figsize=(24, 6))\n",
    "\n",
    "# Color the bars (orange for numeric columns, blue for others)\n",
    "colors = ['orange' if feature in df_combined.columns else 'blue' for feature in feature_importance_df['Feature']]\n",
    "\n",
    "# Plot the bars\n",
    "bars = plt.bar(feature_importance_df['Feature'], feature_importance_df['Importance'], color=colors)\n",
    "\n",
    "# Set x-labels to blank for non-numeric features (blue)\n",
    "for i, feature in enumerate(feature_importance_df['Feature']):\n",
    "    if feature not in numeric_columns:\n",
    "        bars[i].set_label(' ')  # Set blank label for blue bars\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Labels and title\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.title(\"Feature Importance with Highlighted Numeric Features (Importance > 0)\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bY1w1ob-aSYB"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combine the numeric columns and new features into a DataFrame\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = df_combined.corr()\n",
    "\n",
    "# Set up the figure size and seaborn's heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# Set the title for the heatmap\n",
    "plt.title(\"Correlation Heatmap of Selected Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvePtZEob7ED"
   },
   "outputs": [],
   "source": [
    "# Ensure that 'Features' contains the fingerprint data and combine it\n",
    "X = np.vstack(df[\"Features1\"].values)  # Features from RDKit or other representation\n",
    "X_combined = np.hstack((df_combined.values, X))  # Combine the features\n",
    "\n",
    "# Target variable\n",
    "y = df['count_binned_custom'].values  # Target labels\n",
    "\n",
    "# Train-test split (optional but common to have)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=200, min_samples_split=10,\n",
    "                             class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=10, scoring=\"accuracy\")\n",
    "\n",
    "# Print results\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean accuracy:\", scores.mean())\n",
    "print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxsCteCZV0sq"
   },
   "source": [
    "##Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1iw1R_e6V0Zi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from rdkit import Chem\n",
    "\n",
    "# Assuming the \"Features\" column is already created\n",
    "X = np.vstack(df[\"Features1\"].values)  # Features\n",
    "y = df[\"count\"].values  # Continuous target (e.g., infection count, etc.)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model using training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the trained model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform 5-fold cross-validation using linear regression\n",
    "scores = cross_val_score(model, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(\"Cross-validation MSE scores:\", -scores)\n",
    "print(\"Mean MSE:\", -scores.mean())\n",
    "print(\"Standard deviation:\", scores.std())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
