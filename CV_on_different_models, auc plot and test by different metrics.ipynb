{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZLu0CqZtsPo",
    "outputId": "04a07b8a-dc28-4566-f141-799c62cbde1e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from deepchem.feat import PagtnMolGraphFeaturizer\n",
    "\n",
    "\n",
    "mol = pd.read_csv(\"/result.csv\")\n",
    "\n",
    "smiles = mol.drop_duplicates(subset = ['Pubid'], keep='first')\n",
    "smiles = smiles['Smiles']\n",
    "\n",
    "smiles_counts = mol['Smiles'].value_counts().reset_index()\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HCz8kM-IuDrp",
    "outputId": "958af586-c6e4-4431-8d87-0e2af811e664"
   },
   "outputs": [],
   "source": [
    "from deepchem.feat import PagtnMolGraphFeaturizer\n",
    "\n",
    "\n",
    "# Initialize the featurizer\n",
    "featurizer = PagtnMolGraphFeaturizer(max_length=5)\n",
    "\n",
    "# Apply the featurizer to the SMILES series\n",
    "featurized_data = featurizer.featurize(smiles.tolist())\n",
    "\n",
    "from deepchem.feat import PagtnMolGraphFeaturizer\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the featurizer\n",
    "featurizer = PagtnMolGraphFeaturizer(max_length=5)\n",
    "\n",
    "# Apply the featurizer to the SMILES series\n",
    "featurized_data = featurizer.featurize(smiles.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vnf7CxOtuGLg",
    "outputId": "f06d5ebf-c559-4fae-d1ae-42359ef0ef87"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, graph_data in enumerate(featurized_data):\n",
    "    result = {\n",
    "        \"SMILES\": smiles.iloc[i],\n",
    "        \"Node Features\": graph_data.node_features.tolist(),  # Convert to list for storage\n",
    "        \"Edge Features\": graph_data.edge_features.tolist(),   # Convert to list for storage\n",
    "        \"Edge Index\": graph_data.edge_index.tolist()         # Convert to list for storage\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "# Convert results to a DataFrame for better visualization or storage\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to a CSV or JSON file if needed\n",
    "results_df.to_csv(\"node_edge_features.csv\", index=False)\n",
    "\n",
    "# Display the results\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lvFiD0znuGvu",
    "outputId": "ff60d8da-d6a3-4e81-814b-dfa72e397137"
   },
   "outputs": [],
   "source": [
    "df = pd.merge(results_df, smiles_counts, left_on='SMILES', right_on='Smiles', how = 'left')\n",
    "(df['count']).describe()\n",
    "\n",
    "unique_interactions = mol.groupby(\"Compound ID\")[\"gene_name\"].nunique()\n",
    "\n",
    "# Display results\n",
    "df_dedup = unique_interactions.reset_index()\n",
    "df['number_dedup'] = df_dedup['gene_name']\n",
    "df['count_binned_custom'] = pd.cut(df['number_dedup'], bins=[0, 20, 60, 115], labels=[0, 1, 2])\n",
    "\n",
    "graphs = []\n",
    "\n",
    "for i, graph_data in enumerate(featurized_data):\n",
    "    # Convert node features and edge indices into PyTorch tensors\n",
    "    x = torch.tensor(graph_data.node_features, dtype=torch.float)  # Node features\n",
    "    edge_index = torch.tensor(graph_data.edge_index, dtype=torch.long).t().contiguous()  # Edge index\n",
    "    edge_attr = torch.tensor(graph_data.edge_features, dtype=torch.float)  # Edge features\n",
    "    edge_index = edge_index.T\n",
    "    y = torch.tensor(df['count_binned_custom'][i], dtype=torch.long)  # Target label\n",
    "\n",
    "    # Create a PyTorch Geometric Data object\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "    print(data)\n",
    "\n",
    "    graphs.append(data)\n",
    "\n",
    "# Create a DataLoader\n",
    "loader = DataLoader(graphs, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMNIOZuYyYvN"
   },
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_edge_features, hidden_channels, num_classes):\n",
    "        super(GAT, self).__init__()\n",
    "        # Use GATConv which supports edge features\n",
    "        self.conv1 = GATConv(num_node_features, hidden_channels, edge_dim=num_edge_features)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels, edge_dim=num_edge_features)\n",
    "        self.lin = torch.nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # First graph convolution layer (with edge attributes)\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Second graph convolution layer (with edge attributes)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Apply global mean pooling to aggregate node features\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Apply dropout for regularization\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        # Linear layer to get final logits\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # LogSoftmax for multi-class classification\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CE9kiFdVeL4I"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import random\n",
    "\n",
    "# Create dataset\n",
    "num_graphs = 172  # Adjust based on dataset size needs\n",
    "# random.seed(5)  # or torch.manual_seed(42) for PyTorch's random number generator\n",
    "# torch.manual_seed(5)\n",
    "# Split into train and test sets\n",
    "train_size = int(0.8 * num_graphs)\n",
    "test_size = num_graphs - train_size\n",
    "train_graphs, test_graphs = random_split(graphs, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_graphs, batch_size=32, shuffle=False)\n",
    "\n",
    "num_edge_features = graphs[0].edge_attr.size(1) if graphs[0].edge_attr is not None else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNi55rhp3EHy"
   },
   "outputs": [],
   "source": [
    "def evaluate_more(model, loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in loader:\n",
    "            out = model(batch)  # Forward pass\n",
    "            pred = out.argmax(dim=1)  # Get the predicted class\n",
    "            y_true.extend(batch.y.cpu().numpy())\n",
    "            y_pred.extend(pred.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')  # Use 'weighted' for multi-class\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')  # Use 'weighted' for multi-class\n",
    "\n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "StUk2e4fGLWF",
    "outputId": "aeb517ae-e707-4f4d-ee30-b1e94b8a7a52"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "# Create a directory to store the saved models\n",
    "os.makedirs(\"cv_models_GAT_new\", exist_ok=True)\n",
    "\n",
    "# Define number of folds\n",
    "num_folds = 5\n",
    "\n",
    "# Convert dataset into indices for k-fold splitting\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results for averaging later\n",
    "accuracies, precisions, recalls = [], [], []\n",
    "\n",
    "# Perform 5-fold Cross-Validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(graphs)):\n",
    "    print(f\"\\nFold {fold + 1}/{num_folds}\")\n",
    "\n",
    "    # Create train and validation sets\n",
    "    train_subset = [graphs[i] for i in train_idx]\n",
    "    val_subset = [graphs[i] for i in val_idx]\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize model (reinitialize for each fold to start fresh)\n",
    "    model = GAT(num_node_features=graphs[0].num_node_features,\n",
    "                num_edge_features=graphs[0].edge_attr.size(1) if graphs[0].edge_attr is not None else 0,\n",
    "                hidden_channels=64,\n",
    "                num_classes=3)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(70):  # Reduce epochs for faster CV\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch)\n",
    "            loss = F.nll_loss(out, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Optionally save after each epoch\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save(model.state_dict(), f\"cv_models_GAT_new/model_fold{fold + 1}_epoch{epoch + 1}.pt\")\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(train_loader):.4f} - Model saved.\")\n",
    "\n",
    "    # Save the model at the end of the fold\n",
    "    torch.save(model.state_dict(), f\"cv_models_GAT_new/model_fold{fold + 1}.pt\")\n",
    "    print(f\"Fold {fold + 1} model saved.\")\n",
    "\n",
    "    # Evaluation on validation set\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            out = model(batch)\n",
    "            _, preds = torch.max(out, 1)\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(batch.y.tolist())\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "    print(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Precision: {precision:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Recall: {recall:.4f}\")\n",
    "\n",
    "    # Store results\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "# Print final averaged results across all folds\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"Average Accuracy: {sum(accuracies) / num_folds:.4f}\")\n",
    "print(f\"Average Precision: {sum(precisions) / num_folds:.4f}\")\n",
    "print(f\"Average Recall: {sum(recalls) / num_folds:.4f}\")\n",
    "\n",
    "# Assuming you want to save the final model on the test data:\n",
    "test_accuracy_gat, test_precision_gat, test_recall_gat = evaluate_more(model, test_loader)\n",
    "print(f\"Final Test Accuracy: {test_accuracy_gat:.4f}\")\n",
    "print(f\"Final Test Precision: {test_precision_gat:.4f}\")\n",
    "print(f\"Final Test Recall: {test_recall_gat:.4f}\")\n",
    "\n",
    "# Save the final model after training\n",
    "torch.save(model.state_dict(), f\"cv_models_GAT_new/final_model.pt\")\n",
    "print(\"Final model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4s6PN--JaHar",
    "outputId": "0275e739-6b47-422e-db5b-e0aa990bf025"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef, jaccard_score\n",
    ")\n",
    "\n",
    "# Define metric computation\n",
    "def compute_metrics(y_true, y_pred, average='macro', num_classes=None):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    jaccard = jaccard_score(y_true, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    # Compute specificity\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "    tn = cm.sum() - (cm.sum(axis=0) + cm.sum(axis=1) - np.diag(cm))\n",
    "    fp = cm.sum(axis=0) - np.diag(cm)\n",
    "    specificity_per_class = tn / (tn + fp + 1e-10)\n",
    "    specificity = specificity_per_class.mean()\n",
    "\n",
    "    return acc, prec, rec, f1, specificity, mcc, jaccard\n",
    "\n",
    "# Initialize\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_metrics = []\n",
    "\n",
    "# CV Evaluation\n",
    "for fold, (_, val_idx) in enumerate(kf.split(graphs)):\n",
    "    print(f\"\\nEvaluating Fold {fold + 1}\")\n",
    "    val_subset = [graphs[i] for i in val_idx]\n",
    "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = GAT(num_node_features=graphs[0].num_node_features,\n",
    "                num_edge_features=graphs[0].edge_attr.size(1) if graphs[0].edge_attr is not None else 0,\n",
    "                hidden_channels=64,\n",
    "                num_classes=3)\n",
    "    model.load_state_dict(torch.load(f\"cv_models_GAT_new/model_fold{fold + 1}.pt\"))\n",
    "    model.eval()\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            out = model(batch)\n",
    "            _, preds = torch.max(out, 1)\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(batch.y.tolist())\n",
    "\n",
    "    metrics = compute_metrics(all_labels, all_preds, average='macro', num_classes=4)\n",
    "    metric_names = ['accuracy', 'precision', 'recall', 'f1', 'specificity', 'mcc', 'jaccard']\n",
    "    fold_metrics.append(dict(Fold=fold + 1, **dict(zip(metric_names, metrics))))\n",
    "\n",
    "# Add averages\n",
    "averages = {name: np.mean([fm[name] for fm in fold_metrics]) for name in metric_names}\n",
    "averages['Fold'] = 'Average'\n",
    "fold_metrics.append(averages)\n",
    "\n",
    "# Save CV results to CSV\n",
    "cv_df = pd.DataFrame(fold_metrics)\n",
    "cv_df.to_csv(\"cv_results_GAT.csv\", index=False)\n",
    "print(\"\\nSaved cross-validation metrics to cv_results_GAT.csv\")\n",
    "\n",
    "# Test Evaluation\n",
    "model.load_state_dict(torch.load(\"cv_models_GAT_new/final_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        out = model(batch)\n",
    "        _, preds = torch.max(out, 1)\n",
    "        all_preds.extend(preds.tolist())\n",
    "        all_labels.extend(batch.y.tolist())\n",
    "\n",
    "test_metrics = compute_metrics(all_labels, all_preds, average='macro', num_classes=4)\n",
    "test_df = pd.DataFrame([dict(zip(metric_names, test_metrics))])\n",
    "test_df.to_csv(\"test_results_GAT.csv\", index=False)\n",
    "print(\"Saved final test metrics to test_results_GAT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Etc7Mh-kNeeR",
    "outputId": "b015a9a4-c9d2-4dc8-ec26-4196c958a9ca"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "# Create a directory to store the saved models\n",
    "os.makedirs(\"cv_models_GCN_new\", exist_ok=True)\n",
    "\n",
    "# Define number of folds\n",
    "num_folds = 5\n",
    "\n",
    "# Convert dataset into indices for k-fold splitting\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results for averaging later\n",
    "accuracies, precisions, recalls = [], [], []\n",
    "\n",
    "# Perform 5-fold Cross-Validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(graphs)):\n",
    "    print(f\"\\nFold {fold + 1}/{num_folds}\")\n",
    "\n",
    "    # Create train and validation sets\n",
    "    train_subset = [graphs[i] for i in train_idx]\n",
    "    val_subset = [graphs[i] for i in val_idx]\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize model (reinitialize for each fold to start fresh)\n",
    "    model = GCN(num_node_features=graphs[0].num_node_features,\n",
    "                hidden_channels=64,\n",
    "                num_classes=3)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(100):  # Adjust epochs as needed\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch)\n",
    "            loss = F.nll_loss(out, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Optionally save after each epoch\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save(model.state_dict(), f\"cv_models_GCN_new/model_fold{fold + 1}_epoch{epoch + 1}.pt\")\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(train_loader):.4f} - Model saved.\")\n",
    "\n",
    "    # Save the model at the end of the fold\n",
    "    torch.save(model.state_dict(), f\"cv_models_GCN_new/model_fold{fold + 1}.pt\")\n",
    "    print(f\"Fold {fold + 1} model saved.\")\n",
    "\n",
    "    # Evaluation on validation set\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            out = model(batch)\n",
    "            _, preds = torch.max(out, 1)\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(batch.y.tolist())\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "    print(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Precision: {precision:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Recall: {recall:.4f}\")\n",
    "\n",
    "    # Store results\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "# Print final averaged results across all folds\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"Average Accuracy: {sum(accuracies) / num_folds:.4f}\")\n",
    "print(f\"Average Precision: {sum(precisions) / num_folds:.4f}\")\n",
    "print(f\"Average Recall: {sum(recalls) / num_folds:.4f}\")\n",
    "\n",
    "# Assuming you want to save the final model on the test data:\n",
    "test_accuracy_gcn, test_precision_gcn, test_recall_gcn = evaluate_more(model, test_loader)\n",
    "print(f\"Final Test Accuracy: {test_accuracy_gcn:.4f}\")\n",
    "print(f\"Final Test Precision: {test_precision_gcn:.4f}\")\n",
    "print(f\"Final Test Recall: {test_recall_gcn:.4f}\")\n",
    "\n",
    "# Save the final model after training\n",
    "torch.save(model.state_dict(), f\"cv_models_GCN_new/final_model.pt\")\n",
    "print(\"Final model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oYn_pIbKsDLK",
    "outputId": "015a62b8-c7bd-419f-e10f-27216d7ac76e"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# all_labels: list[int]\n",
    "# all_probs: np.ndarray of shape (n_samples, num_classes)\n",
    "# num_classes: int\n",
    "\n",
    "# 1. Binarize labels\n",
    "y_true = label_binarize(all_labels, classes=list(range(num_classes)))\n",
    "y_score = all_probs\n",
    "\n",
    "# 2. Compute micro-average ROC\n",
    "fpr_micro, tpr_micro, thresholds_micro = roc_curve(y_true.ravel(), y_score.ravel())\n",
    "roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
    "\n",
    "fpr_dict = {}\n",
    "tpr_dict = {}\n",
    "auc_dict = {}\n",
    "\n",
    "for i in range(num_classes):\n",
    "    fpr_dict[i], tpr_dict[i], _ = roc_curve(y_true[:, i], y_score[:, i])\n",
    "    auc_dict[i] = auc(fpr_dict[i], tpr_dict[i])\n",
    "\n",
    "# 4. 保存为 pickle\n",
    "results = {\n",
    "    'micro': {\n",
    "        'fpr': fpr_micro,\n",
    "        'tpr': tpr_micro,\n",
    "        'thresholds': thresholds_micro,\n",
    "        'auc': roc_auc_micro\n",
    "    },\n",
    "    'per_class': {\n",
    "        'fpr': fpr_dict,\n",
    "        'tpr': tpr_dict,\n",
    "        'auc': auc_dict\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"gat_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "print(\"ROC curve data saved to 'gat_results.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_7wxF9izXeN",
    "outputId": "0b8db390-0b01-4d15-e1ca-812afca4fc4d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to store the micro-average ROC curve data\n",
    "micro_roc_data = {\n",
    "    'FPR_micro': fpr,  # False Positive Rate for micro-average\n",
    "    'TPR_micro': tpr,  # True Positive Rate for micro-average\n",
    "    'AUC_micro': [roc_auc] * len(fpr)  # AUC for micro-average (same value for all FPR/TPR)\n",
    "}\n",
    "\n",
    "# Ensure equal lengths for all columns using zip\n",
    "min_len = min(len(fpr), len(tpr))  # Find the minimum length\n",
    "micro_roc_data = {\n",
    "    'FPR_micro': fpr[:min_len],\n",
    "    'TPR_micro': tpr[:min_len],\n",
    "    'AUC_micro': [roc_auc] * min_len\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "micro_roc_df = pd.DataFrame(micro_roc_data)\n",
    "\n",
    "# Save to CSV\n",
    "micro_roc_df.to_csv(\"GCN_Micro_ROC_Curve_Data.csv\", index=False)\n",
    "print(\"Micro-average ROC data saved to GCN_Micro_ROC_Curve_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "# Create a directory to store the saved models\n",
    "os.makedirs(\"cv_models_GAT_new\", exist_ok=True)\n",
    "\n",
    "# Define number of folds\n",
    "num_folds = 5\n",
    "\n",
    "# Convert dataset into indices for k-fold splitting\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results for averaging later\n",
    "accuracies, precisions, recalls = [], [], []\n",
    "\n",
    "# Perform 5-fold Cross-Validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(graphs)):\n",
    "    print(f\"\\nFold {fold + 1}/{num_folds}\")\n",
    "\n",
    "    # Create train and validation sets\n",
    "    train_subset = [graphs[i] for i in train_idx]\n",
    "    val_subset = [graphs[i] for i in val_idx]\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize model (reinitialize for each fold to start fresh)\n",
    "    model = GAT(num_node_features=graphs[0].num_node_features,\n",
    "                num_edge_features=graphs[0].edge_attr.size(1) if graphs[0].edge_attr is not None else 0,\n",
    "                hidden_channels=64,\n",
    "                num_classes=3)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(70):  # Reduce epochs for faster CV\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch)\n",
    "            loss = F.nll_loss(out, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Optionally save after each epoch\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save(model.state_dict(), f\"cv_models_GAT_new/model_fold{fold + 1}_epoch{epoch + 1}.pt\")\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(train_loader):.4f} - Model saved.\")\n",
    "\n",
    "    # Save the model at the end of the fold\n",
    "    torch.save(model.state_dict(), f\"cv_models_GAT_new/model_fold{fold + 1}.pt\")\n",
    "    print(f\"Fold {fold + 1} model saved.\")\n",
    "\n",
    "    # Evaluation on validation set\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            out = model(batch)\n",
    "            _, preds = torch.max(out, 1)\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(batch.y.tolist())\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "    print(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Precision: {precision:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Recall: {recall:.4f}\")\n",
    "\n",
    "    # Store results\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "# Print final averaged results across all folds\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"Average Accuracy: {sum(accuracies) / num_folds:.4f}\")\n",
    "print(f\"Average Precision: {sum(precisions) / num_folds:.4f}\")\n",
    "print(f\"Average Recall: {sum(recalls) / num_folds:.4f}\")\n",
    "\n",
    "# Assuming you want to save the final model on the test data:\n",
    "test_accuracy_gat, test_precision_gat, test_recall_gat = evaluate_more(model, test_loader)\n",
    "print(f\"Final Test Accuracy: {test_accuracy_gat:.4f}\")\n",
    "print(f\"Final Test Precision: {test_precision_gat:.4f}\")\n",
    "print(f\"Final Test Recall: {test_recall_gat:.4f}\")\n",
    "\n",
    "# Save the final model after training\n",
    "torch.save(model.state_dict(), f\"cv_models_GAT_new/final_model.pt\")\n",
    "print(\"Final model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gwDAPpC12Tzj",
    "outputId": "afcbf9e8-a89a-4e6e-c1f3-49353f1b28eb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# Function to extract features from a graph\n",
    "def extract_graph_features(graph):\n",
    "    node_features = graph.x.numpy()  # Convert node features to NumPy\n",
    "    edge_features = graph.edge_attr.numpy() if graph.edge_attr is not None else np.zeros((graph.edge_index.shape[1], 1))  # Handle missing edge features\n",
    "\n",
    "    # Aggregate statistics\n",
    "    node_mean = np.mean(node_features, axis=0)\n",
    "    node_max = np.max(node_features, axis=0)\n",
    "    node_std = np.std(node_features, axis=0)\n",
    "\n",
    "    edge_mean = np.mean(edge_features, axis=0)\n",
    "    edge_max = np.max(edge_features, axis=0)\n",
    "    edge_std = np.std(edge_features, axis=0)\n",
    "\n",
    "    # Combine all features\n",
    "    feature_vector = np.concatenate([node_mean, node_max, node_std, edge_mean, edge_max, edge_std])\n",
    "    return feature_vector, graph.y.item()  # Extract label\n",
    "\n",
    "# Extract features for training and testing sets\n",
    "X_train, y_train = zip(*[extract_graph_features(graph) for graph in train_graphs])\n",
    "X_test, y_test = zip(*[extract_graph_features(graph) for graph in test_graphs])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_train, X_test = np.array(X_train), np.array(X_test)\n",
    "y_train, y_test = np.array(y_train), np.array(y_test)\n",
    "\n",
    "# Now X_train and X_test can be used in ML models like RandomForest, SVM, etc.\n",
    "print(f\"Extracted Features Shape - Train: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4WeLvTfSxOE"
   },
   "source": [
    "# Other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9KoN83bSyjZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef, jaccard_score\n",
    "\n",
    "def compute_metrics(y_true, y_pred, average='macro', num_classes=None):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    jaccard = jaccard_score(y_true, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    # Compute Specificity manually\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "    tn = cm.sum() - (cm.sum(axis=0) + cm.sum(axis=1) - np.diag(cm))\n",
    "    fp = cm.sum(axis=0) - np.diag(cm)\n",
    "    specificity_per_class = tn / (tn + fp + 1e-10)  # Avoid division by zero\n",
    "    specificity = specificity_per_class.mean()\n",
    "\n",
    "    return acc, prec, rec, f1, specificity, mcc, jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3aKNJM0Owhc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef, jaccard_score\n",
    ")\n",
    "\n",
    "# Define metric computation\n",
    "def compute_metrics(y_true, y_pred, average='macro', num_classes=None):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    jaccard = jaccard_score(y_true, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    # Compute specificity\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "    tn = cm.sum() - (cm.sum(axis=0) + cm.sum(axis=1) - np.diag(cm))\n",
    "    fp = cm.sum(axis=0) - np.diag(cm)\n",
    "    specificity_per_class = tn / (tn + fp + 1e-10)\n",
    "    specificity = specificity_per_class.mean()\n",
    "\n",
    "    return acc, prec, rec, f1, specificity, mcc, jaccard\n",
    "\n",
    "# Initialize\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_metrics = []\n",
    "\n",
    "# CV Evaluation\n",
    "for fold, (_, val_idx) in enumerate(kf.split(graphs)):\n",
    "    print(f\"\\nEvaluating Fold {fold + 1}\")\n",
    "    val_subset = [graphs[i] for i in val_idx]\n",
    "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = GAT(num_node_features=graphs[0].num_node_features,\n",
    "                num_edge_features=graphs[0].edge_attr.size(1) if graphs[0].edge_attr is not None else 0,\n",
    "                hidden_channels=64,\n",
    "                num_classes=4)\n",
    "    model.load_state_dict(torch.load(f\"cv_models_GAT_new/model_fold{fold + 1}.pt\"))\n",
    "    model.eval()\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            out = model(batch)\n",
    "            _, preds = torch.max(out, 1)\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(batch.y.tolist())\n",
    "\n",
    "    metrics = compute_metrics(all_labels, all_preds, average='macro', num_classes=4)\n",
    "    metric_names = ['accuracy', 'precision', 'recall', 'f1', 'specificity', 'mcc', 'jaccard']\n",
    "    fold_metrics.append(dict(Fold=fold + 1, **dict(zip(metric_names, metrics))))\n",
    "\n",
    "# Add averages\n",
    "averages = {name: np.mean([fm[name] for fm in fold_metrics]) for name in metric_names}\n",
    "averages['Fold'] = 'Average'\n",
    "fold_metrics.append(averages)\n",
    "\n",
    "# Save CV results to CSV\n",
    "cv_df = pd.DataFrame(fold_metrics)\n",
    "cv_df.to_csv(\"cv_results_GAT.csv\", index=False)\n",
    "print(\"\\nSaved cross-validation metrics to cv_results_GAT.csv\")\n",
    "\n",
    "# Test Evaluation\n",
    "model.load_state_dict(torch.load(\"cv_models_GAT_new/final_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        out = model(batch)\n",
    "        _, preds = torch.max(out, 1)\n",
    "        all_preds.extend(preds.tolist())\n",
    "        all_labels.extend(batch.y.tolist())\n",
    "\n",
    "test_metrics = compute_metrics(all_labels, all_preds, average='macro', num_classes=4)\n",
    "test_df = pd.DataFrame([dict(zip(metric_names, test_metrics))])\n",
    "test_df.to_csv(\"test_results_GAT.csv\", index=False)\n",
    "print(\"Saved final test metrics to test_results_GAT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UIzA-lvmPEnZ",
    "outputId": "5b19dca7-09e9-4173-8534-cba6deb121b6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef, jaccard_score\n",
    ")\n",
    "\n",
    "# Define metric computation function\n",
    "def compute_metrics(y_true, y_pred, average='macro', num_classes=None):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    jaccard = jaccard_score(y_true, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    # Compute specificity\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "    tn = cm.sum() - (cm.sum(axis=0) + cm.sum(axis=1) - np.diag(cm))\n",
    "    fp = cm.sum(axis=0) - np.diag(cm)\n",
    "    specificity_per_class = tn / (tn + fp + 1e-10)\n",
    "    specificity = specificity_per_class.mean()\n",
    "\n",
    "    return acc, prec, rec, f1, specificity, mcc, jaccard\n",
    "\n",
    "# Set up\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_metrics_gcn = []\n",
    "\n",
    "# CV evaluation for GCN\n",
    "for fold, (_, val_idx) in enumerate(kf.split(graphs)):\n",
    "    print(f\"\\nEvaluating GCN Fold {fold + 1}\")\n",
    "    val_subset = [graphs[i] for i in val_idx]\n",
    "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = GCN(num_node_features=graphs[0].num_node_features,\n",
    "                hidden_channels=64,\n",
    "                num_classes=3)\n",
    "    model.load_state_dict(torch.load(f\"cv_models_GCN_new/model_fold{fold + 1}.pt\"))\n",
    "    model.eval()\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            out = model(batch)\n",
    "            _, preds = torch.max(out, 1)\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(batch.y.tolist())\n",
    "\n",
    "    metrics = compute_metrics(all_labels, all_preds, average='macro', num_classes=4)\n",
    "    metric_names = ['accuracy', 'precision', 'recall', 'f1', 'specificity', 'mcc', 'jaccard']\n",
    "    fold_metrics_gcn.append(dict(Fold=fold + 1, **dict(zip(metric_names, metrics))))\n",
    "\n",
    "# Add average row\n",
    "averages = {name: np.mean([fm[name] for fm in fold_metrics_gcn]) for name in metric_names}\n",
    "averages['Fold'] = 'Average'\n",
    "fold_metrics_gcn.append(averages)\n",
    "\n",
    "# Save CV results to CSV\n",
    "cv_df_gcn = pd.DataFrame(fold_metrics_gcn)\n",
    "cv_df_gcn.to_csv(\"cv_results_GCN.csv\", index=False)\n",
    "print(\"Saved GCN CV results to cv_results_GCN.csv\")\n",
    "\n",
    "# Test evaluation\n",
    "model.load_state_dict(torch.load(\"cv_models_GCN_new/final_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        out = model(batch)\n",
    "        _, preds = torch.max(out, 1)\n",
    "        all_preds.extend(preds.tolist())\n",
    "        all_labels.extend(batch.y.tolist())\n",
    "\n",
    "test_metrics = compute_metrics(all_labels, all_preds, average='macro', num_classes=4)\n",
    "test_df = pd.DataFrame([dict(zip(metric_names, test_metrics))])\n",
    "test_df.to_csv(\"test_results_GCN.csv\", index=False)\n",
    "print(\"Saved GCN test results to test_results_GCN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEedHyARPpuv",
    "outputId": "817748c5-4b1c-4d9a-cb61-8fb28a170c27"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib  # NEW: for saving models\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef, jaccard_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scipy.stats import skew, kurtosis\n",
    "# Function to extract features from a graph\n",
    "def extract_graph_features(graph):\n",
    "    node_features = graph.x.numpy()\n",
    "    edge_features = graph.edge_attr.numpy() if graph.edge_attr is not None else np.zeros((graph.edge_index.shape[1], 1))\n",
    "\n",
    "    node_mean = np.mean(node_features, axis=0)\n",
    "    node_skew = skew(node_features, axis=0)\n",
    "    node_kurtosis = kurtosis(node_features, axis=0)\n",
    "    node_std = np.std(node_features, axis=0)\n",
    "\n",
    "    edge_mean = np.mean(edge_features, axis=0)\n",
    "    edge_skew = skew(edge_features, axis=0)\n",
    "    edge_kurtosis = kurtosis(edge_features, axis=0)\n",
    "    edge_std = np.std(edge_features, axis=0)\n",
    "\n",
    "    feature_vector = np.concatenate([node_mean, node_std, edge_mean, edge_std])\n",
    "    feature_vector = np.concatenate([feature_vector, node_skew, node_kurtosis, edge_skew, edge_kurtosis])\n",
    "    return feature_vector, graph.y.item()\n",
    "\n",
    "# Extract features\n",
    "X, y = zip(*[extract_graph_features(graph) for graph in graphs])\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "X = np.nan_to_num(X, nan=0.0)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=50, max_depth=50, min_samples_split=5, class_weight=\"balanced\", random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, max_depth=5, random_state=42),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", C=0.01, gamma=0.001, probability=True),\n",
    "    \"Logistic Regression\": LogisticRegression(penalty='l2', C=0.01, max_iter=30, solver=\"lbfgs\", random_state=42),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=20)\n",
    "cv_results, test_results = [], []\n",
    "\n",
    "# Metric function\n",
    "def compute_metrics(y_true, y_pred, average='macro', num_classes=None):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    jaccard = jaccard_score(y_true, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "    tn = cm.sum() - (cm.sum(axis=0) + cm.sum(axis=1) - np.diag(cm))\n",
    "    fp = cm.sum(axis=0) - np.diag(cm)\n",
    "    specificity_per_class = tn / (tn + fp + 1e-10)\n",
    "    specificity = specificity_per_class.mean()\n",
    "    return acc, prec, rec, f1, specificity, mcc, jaccard\n",
    "\n",
    "def apply_smoothing(values, alpha=0.1):\n",
    "    smoothed_values = [values[0]]\n",
    "    for val in values[1:]:\n",
    "        smoothed_values.append(alpha * val + (1 - alpha) * smoothed_values[-1])\n",
    "    return smoothed_values\n",
    "\n",
    "# Save directory\n",
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)  # NEW: create base directory\n",
    "\n",
    "# CV loop\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    accuracies, precisions, recalls, f1_scores, specificities, mccs, jaccards = [], [], [], [], [], [], []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"  Fold {fold+1}/5\")\n",
    "\n",
    "        X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred_fold = model.predict(X_val_fold)\n",
    "\n",
    "        # NEW: Save model\n",
    "        model_dir = os.path.join(save_dir, name.replace(\" \", \"_\"))\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        model_path = os.path.join(model_dir, f\"fold_{fold+1}.joblib\")\n",
    "        joblib.dump(model, model_path)\n",
    "\n",
    "        metrics = compute_metrics(y_val_fold, y_pred_fold, num_classes=len(np.unique(y_train)))\n",
    "        accuracies.append(metrics[0])\n",
    "        precisions.append(metrics[1])\n",
    "        recalls.append(metrics[2])\n",
    "        f1_scores.append(metrics[3])\n",
    "        specificities.append(metrics[4])\n",
    "        mccs.append(metrics[5])\n",
    "        jaccards.append(metrics[6])\n",
    "\n",
    "    smoothed_accuracies = apply_smoothing(accuracies)\n",
    "    smoothed_precisions = apply_smoothing(precisions)\n",
    "    smoothed_recalls = apply_smoothing(recalls)\n",
    "    smoothed_f1_scores = apply_smoothing(f1_scores)\n",
    "    smoothed_specificities = apply_smoothing(specificities)\n",
    "    smoothed_mccs = apply_smoothing(mccs)\n",
    "    smoothed_jaccards = apply_smoothing(jaccards)\n",
    "\n",
    "    cv_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Smoothed Average Accuracy (CV)\": np.mean(smoothed_accuracies),\n",
    "        \"Accuracy Std Dev (CV)\": np.std(smoothed_accuracies),\n",
    "        \"Smoothed Average Precision (CV)\": np.mean(smoothed_precisions),\n",
    "        \"Smoothed Average Recall (CV)\": np.mean(smoothed_recalls),\n",
    "        \"Smoothed Average F1 Score (CV)\": np.mean(smoothed_f1_scores),\n",
    "        \"Smoothed Average Specificity (CV)\": np.mean(smoothed_specificities),\n",
    "        \"Smoothed Average MCC (CV)\": np.mean(smoothed_mccs),\n",
    "        \"Smoothed Average Jaccard (CV)\": np.mean(smoothed_jaccards)\n",
    "    })\n",
    "\n",
    "    # Test set\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    test_metrics = compute_metrics(y_test, y_pred_test, num_classes=len(np.unique(y_train)))\n",
    "    smoothed_test_specificity = apply_smoothing([test_metrics[4]])[0]\n",
    "\n",
    "    test_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Test Accuracy\": test_metrics[0],\n",
    "        \"Test Precision\": test_metrics[1],\n",
    "        \"Test Recall\": test_metrics[2],\n",
    "        \"Test F1 Score\": test_metrics[3],\n",
    "        \"Smoothed Test Specificity\": smoothed_test_specificity,\n",
    "        \"Test MCC\": test_metrics[5],\n",
    "        \"Test Jaccard\": test_metrics[6]\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "df_cv_results = pd.DataFrame(cv_results)\n",
    "df_test_results = pd.DataFrame(test_results)\n",
    "df_cv_results.to_csv(\"cv_results_smoothed.csv\", index=False)\n",
    "df_test_results.to_csv(\"test_results_smoothed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aWZ05vln4bu8",
    "outputId": "ca230ed4-f9b1-4f21-dee1-5532d8bf2879"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef, jaccard_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Function to extract features from a graph\n",
    "def extract_graph_features(graph):\n",
    "    node_features = graph.x.numpy()\n",
    "    edge_features = graph.edge_attr.numpy() if graph.edge_attr is not None else np.zeros((graph.edge_index.shape[1], 1))\n",
    "\n",
    "    # Aggregate statistics\n",
    "    node_mean = np.mean(node_features, axis=0)\n",
    "    node_max = np.max(node_features, axis=0)\n",
    "    node_std = np.std(node_features, axis=0)\n",
    "\n",
    "    edge_mean = np.mean(edge_features, axis=0)\n",
    "    edge_max = np.max(edge_features, axis=0)\n",
    "    edge_std = np.std(edge_features, axis=0)\n",
    "\n",
    "    # Combine all features\n",
    "    feature_vector = np.concatenate([node_mean, node_max, node_std, edge_mean, edge_max, edge_std])\n",
    "    return feature_vector, graph.y.item()\n",
    "\n",
    "# Extract features\n",
    "X, y = zip(*[extract_graph_features(graph) for graph in graphs])\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Split data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
    "\n",
    "# Define models to compare\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=200, min_samples_split=5, class_weight=\"balanced\", random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\"),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, solver=\"lbfgs\", random_state=42),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Define 5-fold CV\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=20)\n",
    "\n",
    "# Store results for CV and test set\n",
    "cv_results = []\n",
    "test_results = []\n",
    "\n",
    "# Function to compute metrics\n",
    "def compute_metrics(y_true, y_pred, average='macro', num_classes=None):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    jaccard = jaccard_score(y_true, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    # Compute Specificity manually\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "    tn = cm.sum() - (cm.sum(axis=0) + cm.sum(axis=1) - np.diag(cm))\n",
    "    fp = cm.sum(axis=0) - np.diag(cm)\n",
    "    specificity_per_class = tn / (tn + fp + 1e-10)  # Avoid division by zero\n",
    "    specificity = specificity_per_class.mean()\n",
    "\n",
    "    return acc, prec, rec, f1, specificity, mcc, jaccard\n",
    "\n",
    "def apply_smoothing(values, alpha=0.1):\n",
    "    smoothed_values = [values[0]]  # Initialize with the first value\n",
    "    for val in values[1:]:\n",
    "        smoothed_values.append(alpha * val + (1 - alpha) * smoothed_values[-1])  # Exponential smoothing\n",
    "    return smoothed_values\n",
    "\n",
    "# Perform cross-validation and evaluate on test set\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "\n",
    "    accuracies, precisions, recalls, f1_scores, specificities, mccs, jaccards = [], [], [], [], [], [], []\n",
    "\n",
    "    # Cross-validation\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"  Fold {fold+1}/{num_folds}\")\n",
    "\n",
    "        X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "    #     model.fit(X_train_fold, y_train_fold)\n",
    "    #     y_pred_fold = model.predict(X_val_fold)\n",
    "\n",
    "    #     # Compute metrics for validation fold\n",
    "    #     metrics = compute_metrics(y_val_fold, y_pred_fold, num_classes=len(np.unique(y_train)))\n",
    "\n",
    "    #     # Append to lists\n",
    "    #     accuracies.append(metrics[0])\n",
    "    #     precisions.append(metrics[1])\n",
    "    #     recalls.append(metrics[2])\n",
    "    #     f1_scores.append(metrics[3])\n",
    "    #     specificities.append(metrics[4])\n",
    "    #     mccs.append(metrics[5])\n",
    "    #     jaccards.append(metrics[6])\n",
    "\n",
    "    # # Apply smoothing to each metric after cross-validation\n",
    "    # smoothed_accuracies = apply_smoothing(accuracies)\n",
    "    # smoothed_precisions = apply_smoothing(precisions)\n",
    "    # smoothed_recalls = apply_smoothing(recalls)\n",
    "    # smoothed_f1_scores = apply_smoothing(f1_scores)\n",
    "    # smoothed_specificities = apply_smoothing(specificities)\n",
    "    # smoothed_mccs = apply_smoothing(mccs)\n",
    "    # smoothed_jaccards = apply_smoothing(jaccards)\n",
    "\n",
    "    # # After cross-validation, calculate averages for smoothed metrics\n",
    "    # cv_results.append({\n",
    "    #     \"Model\": name,\n",
    "    #     \"Smoothed Average Accuracy (CV)\": np.mean(smoothed_accuracies),\n",
    "    #     \"Accuracy Std Dev (CV)\": np.std(smoothed_accuracies),\n",
    "    #     \"Smoothed Average Precision (CV)\": np.mean(smoothed_precisions),\n",
    "    #     \"Smoothed Average Recall (CV)\": np.mean(smoothed_recalls),\n",
    "    #     \"Smoothed Average F1 Score (CV)\": np.mean(smoothed_f1_scores),\n",
    "    #     \"Smoothed Average Specificity (CV)\": np.mean(smoothed_specificities),\n",
    "    #     \"Smoothed Average MCC (CV)\": np.mean(smoothed_mccs),\n",
    "    #     \"Smoothed Average Jaccard (CV)\": np.mean(smoothed_jaccards)\n",
    "    # })\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    # Compute metrics on the test set\n",
    "    test_metrics = compute_metrics(y_test, y_pred_test, num_classes=len(np.unique(y_train)))\n",
    "\n",
    "    # Apply smoothing to the test specificity\n",
    "    smoothed_test_specificity = apply_smoothing([test_metrics[4]])[0]\n",
    "\n",
    "    test_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Test Accuracy\": test_metrics[0],\n",
    "        \"Test Precision\": test_metrics[1],\n",
    "        \"Test Recall\": test_metrics[2],\n",
    "        \"Test F1 Score\": test_metrics[3],\n",
    "        \"Smoothed Test Specificity\": smoothed_test_specificity,\n",
    "        \"Test MCC\": test_metrics[5],\n",
    "        \"Test Jaccard\": test_metrics[6]\n",
    "    })\n",
    "\n",
    "# Convert to DataFrames\n",
    "# df_cv_results = pd.DataFrame(cv_results)\n",
    "df_test_results = pd.DataFrame(test_results)\n",
    "\n",
    "# Display results\n",
    "# print(\"\\nCross-Validation Results:\")\n",
    "# print(df_cv_results)\n",
    "\n",
    "print(\"\\nTest Set Results:\")\n",
    "print(df_test_results)\n",
    "\n",
    "# Save both CV results and test results to CSV\n",
    "# df_cv_results.to_csv(\"cv_results_smoothed.csv\", index=False)\n",
    "df_test_results.to_csv(\"test_results_smoothed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-AoyZkGsNyx",
    "outputId": "0c2eab70-65fa-4b41-c524-2aeff115f988"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef, jaccard_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Function to extract features from a graph\n",
    "def extract_graph_features(graph):\n",
    "    node_features = graph.x.numpy()\n",
    "    edge_features = graph.edge_attr.numpy() if graph.edge_attr is not None else np.zeros((graph.edge_index.shape[1], 1))\n",
    "\n",
    "    node_mean = np.mean(node_features, axis=0)\n",
    "    node_max = np.max(node_features, axis=0)\n",
    "    node_std = np.std(node_features, axis=0)\n",
    "\n",
    "    edge_mean = np.mean(edge_features, axis=0)\n",
    "    edge_max = np.max(edge_features, axis=0)\n",
    "    edge_std = np.std(edge_features, axis=0)\n",
    "\n",
    "    feature_vector = np.concatenate([node_mean, node_max, node_std, edge_mean, edge_max, edge_std])\n",
    "    return feature_vector, graph.y.item()\n",
    "\n",
    "# Extract features\n",
    "X, y = zip(*[extract_graph_features(graph) for graph in graphs])\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Split data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=200, min_samples_split=5, class_weight=\"balanced\", random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\"),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, solver=\"lbfgs\", random_state=42),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Cross-validation setup\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=20)\n",
    "\n",
    "# Create empty dictionary to store fold results for each metric\n",
    "fold_accuracies = {}\n",
    "fold_precisions = {}\n",
    "fold_recalls = {}\n",
    "fold_f1_scores = {}\n",
    "fold_specificities = {}\n",
    "fold_mccs = {}\n",
    "fold_jaccards = {}\n",
    "\n",
    "# Function to compute metrics\n",
    "def compute_metrics(y_true, y_pred, average='macro', num_classes=None):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    jaccard = jaccard_score(y_true, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "    tn = cm.sum() - (cm.sum(axis=0) + cm.sum(axis=1) - np.diag(cm))\n",
    "    fp = cm.sum(axis=0) - np.diag(cm)\n",
    "    specificity_per_class = tn / (tn + fp + 1e-10)\n",
    "    specificity = specificity_per_class.mean()\n",
    "\n",
    "    return acc, prec, rec, f1, specificity, mcc, jaccard\n",
    "\n",
    "# Start training and cross-validation\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "\n",
    "    accs, precs, recs, f1s, specs, mccs, jaccs = [], [], [], [], [], [], []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"  Fold {fold+1}/{num_folds}\")\n",
    "        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "        y_pred_fold = model.predict(X_fold_val)\n",
    "\n",
    "        metrics = compute_metrics(y_fold_val, y_pred_fold, num_classes=len(np.unique(y_train)))\n",
    "\n",
    "        accs.append(metrics[0])\n",
    "        precs.append(metrics[1])\n",
    "        recs.append(metrics[2])\n",
    "        f1s.append(metrics[3])\n",
    "        specs.append(metrics[4])\n",
    "        mccs.append(metrics[5])\n",
    "        jaccs.append(metrics[6])\n",
    "\n",
    "    # Save fold-by-fold results\n",
    "    fold_accuracies[name] = accs\n",
    "    fold_precisions[name] = precs\n",
    "    fold_recalls[name] = recs\n",
    "    fold_f1_scores[name] = f1s\n",
    "    fold_specificities[name] = specs\n",
    "    fold_mccs[name] = mccs\n",
    "    fold_jaccards[name] = jaccs\n",
    "\n",
    "# Create DataFrames for each metric\n",
    "df_accuracy = pd.DataFrame(fold_accuracies, index=[f\"CV{i+1}\" for i in range(num_folds)]).T\n",
    "df_precision = pd.DataFrame(fold_precisions, index=[f\"CV{i+1}\" for i in range(num_folds)]).T\n",
    "df_recall = pd.DataFrame(fold_recalls, index=[f\"CV{i+1}\" for i in range(num_folds)]).T\n",
    "df_f1 = pd.DataFrame(fold_f1_scores, index=[f\"CV{i+1}\" for i in range(num_folds)]).T\n",
    "df_specificity = pd.DataFrame(fold_specificities, index=[f\"CV{i+1}\" for i in range(num_folds)]).T\n",
    "df_mcc = pd.DataFrame(fold_mccs, index=[f\"CV{i+1}\" for i in range(num_folds)]).T\n",
    "df_jaccard = pd.DataFrame(fold_jaccards, index=[f\"CV{i+1}\" for i in range(num_folds)]).T\n",
    "\n",
    "# Display tables\n",
    "print(\"\\nAccuracy per fold:\")\n",
    "print(df_accuracy)\n",
    "\n",
    "print(\"\\nPrecision per fold:\")\n",
    "print(df_precision)\n",
    "\n",
    "print(\"\\nRecall per fold:\")\n",
    "print(df_recall)\n",
    "\n",
    "print(\"\\nF1 Score per fold:\")\n",
    "print(df_f1)\n",
    "\n",
    "print(\"\\nSpecificity per fold:\")\n",
    "print(df_specificity)\n",
    "\n",
    "print(\"\\nMCC per fold:\")\n",
    "print(df_mcc)\n",
    "\n",
    "print(\"\\nJaccard per fold:\")\n",
    "print(df_jaccard)\n",
    "\n",
    "# Optionally, save them\n",
    "df_accuracy.to_csv(\"accuracy_folds.csv\")\n",
    "df_precision.to_csv(\"precision_folds.csv\")\n",
    "df_recall.to_csv(\"recall_folds.csv\")\n",
    "df_f1.to_csv(\"f1_folds.csv\")\n",
    "df_specificity.to_csv(\"specificity_folds.csv\")\n",
    "df_mcc.to_csv(\"mcc_folds.csv\")\n",
    "df_jaccard.to_csv(\"jaccard_folds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 928
    },
    "id": "mxtmkscZt7uK",
    "outputId": "22f03448-1ec8-4c41-c3d7-cc8f34f3fa19"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from itertools import cycle\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=200, min_samples_split=5, class_weight=\"balanced\", random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", probability=True),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, solver=\"lbfgs\", random_state=42),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# 获取类别信息\n",
    "classes = np.unique(y_train)\n",
    "y_test_bin = label_binarize(y_test, classes=classes)\n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = cycle(['palegreen', 'peru', 'blueviolet', 'skyblue', 'orchid'])\n",
    "\n",
    "for (name, model), color in zip(models.items(), colors):\n",
    "    print(f\"Training {name} on full training set...\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    if not hasattr(model, \"predict_proba\"):\n",
    "        print(f\"  Skipped {name} (no predict_proba method)\")\n",
    "        continue\n",
    "\n",
    "    y_score = model.predict_proba(X_test)\n",
    "\n",
    "    if y_score.shape[1] != n_classes:\n",
    "        print(f\"  Skipped {name} (incomplete class predictions)\")\n",
    "        continue\n",
    "\n",
    "    # Compute micro-average ROC\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin.ravel(), y_score.ravel())\n",
    "    micro_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.plot(fpr, tpr, lw=2, color=color, label=f\"{name} (AUC = {micro_auc:.2f})\")\n",
    "\n",
    "micro_roc_df = pd.read_csv(\"GCN_Micro_ROC_Curve_Data.csv\")\n",
    "fpr_micro = micro_roc_df['FPR_micro']\n",
    "tpr_micro = micro_roc_df['TPR_micro']\n",
    "roc_auc_micro = micro_roc_df['AUC_micro'][0]  # Since AUC is the same for all entries\n",
    "\n",
    "# Plot GCN micro-average ROC curve\n",
    "plt.plot(\n",
    "    fpr_micro,\n",
    "    tpr_micro,\n",
    "    lw=2,\n",
    "    color=\"crimson\",  # Use the last color for GCN\n",
    "    label=f'GCN (Micro-average AUC = {roc_auc_micro:.2f})'\n",
    ")\n",
    "print(f\"GCN Micro-average AUC: {roc_auc_micro:.2f}\")\n",
    "\n",
    "with open('gat_results.pkl', 'rb') as f:\n",
    "    gat_results = pickle.load(f)\n",
    "\n",
    "gat_fpr_micro = gat_results['micro']['fpr']\n",
    "gat_tpr_micro = gat_results['micro']['tpr']\n",
    "gat_micro_auc = gat_results['micro']['auc']\n",
    "\n",
    "# Plot GAT macro-average ROC curve\n",
    "plt.plot(gat_fpr_micro, gat_tpr_micro, color='lightpink', lw=2, label=f\"Micro-average ROC curve (AUC = {roc_auc:.2f})\")\n",
    "print(f\"GAT Macro-average AUC: {gat_micro_auc:.2f}\")\n",
    "\n",
    "# 画对角线\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Micro-average ROC Curve Comparison\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"micro_avg_roc_curve.png\")\n",
    "plt.savefig(\"micro_avg_roc_curve.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "Gxli4NinsOXz",
    "outputId": "28b1a0ec-250f-45bb-9a7d-75298e7d60fc"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Step 1: Zip the folder\n",
    "shutil.make_archive('cv_models_GAT_new', 'zip', 'cv_models_GAT_new')  # (output_name, format, source_folder)\n",
    "\n",
    "# Step 2: Download the zipped folder\n",
    "files.download('cv_models_GAT_new.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "mrw2zP60eZbr",
    "outputId": "22117a99-9a5c-4cc4-8edf-6713b057b434"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Step 1: Zip the folder\n",
    "shutil.make_archive('cv_models_GCN_new', 'zip', 'cv_models_GCN_new')  # (output_name, format, source_folder)\n",
    "\n",
    "# Step 2: Download the zipped folder\n",
    "files.download('cv_models_GCN_new.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "UaUv4uzGedAp",
    "outputId": "c1d35b72-dbb7-4fd4-8cd5-6f81a329ab50"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Step 1: Zip the folder\n",
    "shutil.make_archive('saved_models', 'zip', 'saved_models')  # (output_name, format, source_folder)\n",
    "\n",
    "# Step 2: Download the zipped folder\n",
    "files.download('saved_models.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TkCZBgYXF55I",
    "outputId": "15f80a5e-aa1c-4029-a86e-b1034e26a715"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define your paths\n",
    "zip_path = 'cv_models_GAT_new copy.zip'\n",
    "extract_dir = 'unzipped_models'\n",
    "\n",
    "# Unzip\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(f\"Extracted to: {extract_dir}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
